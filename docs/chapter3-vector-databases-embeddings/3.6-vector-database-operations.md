---
id: vector-database-operations
title: Vector Database Operations and Maintenance
---

## 3.6 Vector Database Operations and Maintenance

Operating vector databases in production requires careful attention to data ingestion, updates, backups, monitoring, and performance optimization. This section covers operational best practices for maintaining reliable, high-performance RAG systems in banking environments.

### Data Ingestion Pipeline

#### Batch Ingestion

**Use Case:** Initial index creation or large-scale updates (e.g., quarterly regulatory document refresh).

**Best Practices:**

**1. Batch Size Optimization:**
```python
def batch_ingest(documents, batch_size=100):
    """Ingest documents in optimized batches."""
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        
        # Generate embeddings in batch
        texts = [doc['content'] for doc in batch]
        embeddings = embedding_model.encode(texts, batch_size=32)
        
        # Prepare vectors with metadata
        vectors = [
            (doc['id'], embedding.tolist(), doc['metadata'])
            for doc, embedding in zip(batch, embeddings)
        ]
        
        # Upsert to vector database
        index.upsert(vectors=vectors)
        
        print(f"Ingested {i + len(batch)}/{len(documents)} documents")
```

**2. Parallel Processing:**
```python
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def parallel_ingest(documents, num_workers=4):
    """Parallelize embedding generation and ingestion."""
    chunks = np.array_split(documents, num_workers)
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [
            executor.submit(batch_ingest, chunk)
            for chunk in chunks
        ]
        
        for future in futures:
            future.result()
```

**3. Error Handling and Retry:**
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def robust_upsert(vectors):
    """Upsert with automatic retry on failure."""
    try:
        index.upsert(vectors=vectors)
    except Exception as e:
        print(f"Upsert failed: {e}. Retrying...")
        raise
```

**Industrial-Grade Aspect:** For banking, implement checkpointing—save progress periodically so failed ingestion jobs can resume from the last checkpoint rather than restarting from scratch.

#### Incremental Updates

**Use Case:** Daily document additions, policy updates, or real-time ingestion.

**Strategies:**

**1. Append-Only Updates:**
```python
def add_new_documents(new_docs):
    """Add new documents without affecting existing ones."""
    embeddings = embedding_model.encode([doc['content'] for doc in new_docs])
    
    vectors = [
        (doc['id'], embedding.tolist(), doc['metadata'])
        for doc, embedding in zip(new_docs, embeddings)
    ]
    
    index.upsert(vectors=vectors)
```

**2. Update Existing Documents:**
```python
def update_document(doc_id, new_content, new_metadata):
    """Update an existing document (delete + re-add)."""
    # Delete old version
    index.delete(ids=[doc_id])
    
    # Add updated version
    embedding = embedding_model.encode([new_content])[0]
    index.upsert(vectors=[(doc_id, embedding.tolist(), new_metadata)])
```

**3. Soft Deletes (Recommended for Audit Trails):**
```python
def soft_delete_document(doc_id):
    """Mark document as deleted without removing from index."""
    # Update metadata to mark as deleted
    index.update(
        id=doc_id,
        set_metadata={"deleted": True, "deleted_at": datetime.now().isoformat()}
    )

def search_active_documents(query_vector):
    """Search only non-deleted documents."""
    return index.query(
        vector=query_vector,
        filter={"deleted": {"$ne": True}},
        top_k=10
    )
```

**Industrial-Grade Aspect:** Banking regulations often require retention of deleted documents for audit purposes. Use soft deletes and periodic archival to cold storage.

### Index Maintenance

#### Rebuilding Indexes

**When to Rebuild:**
*   Embedding model changed
*   Index parameters need optimization
*   Index corruption or performance degradation
*   Major data cleanup (remove obsolete documents)

**Zero-Downtime Rebuild Strategy:**

**1. Blue-Green Deployment:**
```python
def blue_green_index_rebuild():
    """Rebuild index without downtime."""
    # Step 1: Create new index (green)
    green_index = create_new_index(name="banking-docs-green")
    
    # Step 2: Populate green index
    ingest_all_documents(green_index)
    
    # Step 3: Validate green index
    if validate_index_quality(green_index):
        # Step 4: Switch traffic to green
        switch_active_index("banking-docs-green")
        
        # Step 5: Delete old index (blue)
        delete_index("banking-docs-blue")
    else:
        print("Validation failed. Keeping blue index active.")
        delete_index("banking-docs-green")
```

**2. Dual-Write Strategy:**
```python
def dual_write_migration(doc):
    """Write to both old and new indexes during migration."""
    old_index.upsert(doc)
    new_index.upsert(doc)
    
# After migration complete, switch reads to new index
```

#### Compaction and Optimization

**Purpose:** Reclaim space from deleted documents, optimize index structure.

**Qdrant Example:**
```python
from qdrant_client import QdrantClient

client = QdrantClient("localhost", port=6333)

# Optimize collection (compacts segments)
client.update_collection(
    collection_name="banking_docs",
    optimizer_config={
        "deleted_threshold": 0.2,  # Compact when 20% deleted
        "vacuum_min_vector_number": 1000,
        "default_segment_number": 2
    }
)
```

**Pinecone Example:**
```python
# Pinecone automatically handles compaction
# Manual optimization not required
```

### Backup and Disaster Recovery

#### Backup Strategies

**1. Snapshot-Based Backups:**

**Qdrant:**
```python
# Create snapshot
client.create_snapshot(collection_name="banking_docs")

# List snapshots
snapshots = client.list_snapshots(collection_name="banking_docs")

# Restore from snapshot
client.recover_snapshot(
    collection_name="banking_docs",
    snapshot_name="banking_docs-2024-03-15.snapshot"
)
```

**2. Export/Import:**
```python
def export_index_to_file(index, output_path):
    """Export entire index to portable format."""
    vectors = []
    
    # Fetch all vectors (paginated)
    for batch in index.fetch_all(batch_size=1000):
        vectors.extend(batch)
    
    # Save to file
    import pickle
    with open(output_path, 'wb') as f:
        pickle.dump(vectors, f)

def import_index_from_file(index, input_path):
    """Restore index from exported file."""
    import pickle
    with open(input_path, 'rb') as f:
        vectors = pickle.load(f)
    
    # Batch upsert
    batch_ingest(vectors, batch_size=100)
```

**3. Source Data Backup:**
```python
# Best practice: Store original documents + embeddings separately
# Enables rebuilding index with different parameters

def backup_source_data(documents, embeddings, backup_path):
    """Backup source documents and embeddings."""
    import json
    
    backup_data = {
        "documents": documents,
        "embeddings": embeddings.tolist(),
        "model_version": "all-mpnet-base-v2-v1.0",
        "timestamp": datetime.now().isoformat()
    }
    
    with open(backup_path, 'w') as f:
        json.dump(backup_data, f)
```

**Industrial-Grade Aspect:** For banking, implement 3-2-1 backup strategy:
*   **3** copies of data (production + 2 backups)
*   **2** different storage media (local + cloud)
*   **1** off-site backup (different region/availability zone)

#### Disaster Recovery Plan

**RTO (Recovery Time Objective) and RPO (Recovery Point Objective):**

| Tier | RTO | RPO | Strategy |
|------|-----|-----|----------|
| Critical | \<1 hour | \<15 min | Multi-region active-active |
| High | \<4 hours | \<1 hour | Multi-AZ with automated failover |
| Medium | \<24 hours | \<24 hours | Daily backups + manual restore |

**Automated Failover Example:**
```python
import time

def health_check(index_url):
    """Check if vector database is healthy."""
    try:
        response = requests.get(f"{index_url}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

def automatic_failover():
    """Monitor primary and failover to secondary if needed."""
    primary_url = "https://primary.vectordb.bank.com"
    secondary_url = "https://secondary.vectordb.bank.com"
    
    while True:
        if not health_check(primary_url):
            print("Primary unhealthy. Failing over to secondary...")
            update_dns_record(secondary_url)  # Point traffic to secondary
            send_alert("Vector DB failover triggered")
        
        time.sleep(30)  # Check every 30 seconds
```

### Monitoring and Observability

#### Key Metrics to Track

**1. Performance Metrics:**
*   **Query Latency:** p50, p95, p99, p99.9
*   **Throughput:** Queries per second (QPS)
*   **Index Size:** Number of vectors, memory usage
*   **Ingestion Rate:** Documents/vectors added per minute

**2. Quality Metrics:**
*   **Recall@K:** Percentage of relevant results in top K
*   **Query Success Rate:** Percentage of queries returning results
*   **Error Rate:** Failed queries / total queries

**3. Resource Metrics:**
*   **CPU Utilization:** Percentage usage
*   **Memory Usage:** RAM consumption
*   **Disk I/O:** Read/write operations
*   **Network Bandwidth:** Data transfer rates

#### Monitoring Implementation

**Prometheus + Grafana:**
```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Define metrics
query_latency = Histogram('vector_search_latency_seconds', 'Query latency')
query_counter = Counter('vector_search_total', 'Total queries')
index_size = Gauge('vector_index_size', 'Number of vectors in index')

@query_latency.time()
def monitored_search(query_vector):
    """Search with latency tracking."""
    query_counter.inc()
    return index.query(vector=query_vector, top_k=10)

# Update index size periodically
def update_metrics():
    index_size.set(index.describe_index_stats()['total_vector_count'])
```

**Logging Best Practices:**
```python
import logging
import json

logger = logging.getLogger(__name__)

def structured_logging(query, results, latency, user_id):
    """Log queries with structured format for analysis."""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "user_id": user_id,
        "query": query,
        "num_results": len(results),
        "latency_ms": latency * 1000,
        "top_score": results[0]['score'] if results else None
    }
    logger.info(json.dumps(log_entry))
```

**Industrial-Grade Aspect:** For banking, implement comprehensive audit logging:
*   Who queried what, when
*   Which documents were retrieved
*   Access control decisions (allowed/denied)
*   Retention: 7+ years for regulatory compliance

### Performance Optimization

#### Query Optimization

**1. Caching:**
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_search(query_hash, top_k):
    """Cache frequent queries."""
    # Actual search implementation
    return index.query(vector=query_vector, top_k=top_k)

def search_with_cache(query_text, top_k=10):
    query_hash = hashlib.md5(query_text.encode()).hexdigest()
    return cached_search(query_hash, top_k)
```

**2. Connection Pooling:**
```python
from qdrant_client import QdrantClient

# Create connection pool
client = QdrantClient(
    host="localhost",
    port=6333,
    grpc_port=6334,
    prefer_grpc=True,  # Faster than HTTP
    timeout=30
)
```

**3. Batch Queries:**
```python
def batch_search(queries, top_k=10):
    """Process multiple queries in one request."""
    query_embeddings = embedding_model.encode(queries)
    
    results = []
    for embedding in query_embeddings:
        results.append(index.query(vector=embedding.tolist(), top_k=top_k))
    
    return results
```

#### Index Optimization

**1. Right-Size Index Parameters:**
```python
# Tune based on your data size and latency requirements
optimal_params = {
    "hnsw_config": {
        "m": 32,  # Increase for better recall
        "ef_construct": 200,  # Higher for better index quality
    },
    "quantization_config": {
        "scalar": {
            "type": "int8",  # 4x compression
            "quantile": 0.99
        }
    }
}
```

**2. Segment Optimization (Qdrant):**
```python
client.update_collection(
    collection_name="banking_docs",
    optimizer_config={
        "indexing_threshold": 20000,  # Index after 20k vectors
        "max_segment_size": 100000     # Max vectors per segment
    }
)
```

### Cost Optimization

**Strategies:**

1.  **Use Quantization:** Reduce storage costs by 4-10x
2.  **Tiered Storage:** Hot (recent) vs. cold (archive) data
3.  **Auto-Scaling:** Scale down during off-peak hours
4.  **Reserved Capacity:** Commit to long-term usage for discounts (cloud providers)
5.  **Optimize Embeddings:** Use smaller dimension models where acceptable

**Cost Monitoring:**
```python
def estimate_monthly_cost(num_vectors, dimensions, qps):
    """Estimate vector database costs."""
    # Storage cost (example: $0.10 per GB/month)
    storage_gb = (num_vectors * dimensions * 4) / (1024**3)  # 4 bytes per float
    storage_cost = storage_gb * 0.10
    
    # Query cost (example: $0.001 per 1000 queries)
    monthly_queries = qps * 60 * 60 * 24 * 30
    query_cost = (monthly_queries / 1000) * 0.001
    
    total_cost = storage_cost + query_cost
    print(f"Estimated monthly cost: ${total_cost:.2f}")
    return total_cost
```

### Summary

Vector database operations require careful planning and ongoing maintenance. Key takeaways:

*   **Batch ingestion with error handling** ensures reliable data loading
*   **Incremental updates and soft deletes** enable continuous operation
*   **Blue-green deployments** allow zero-downtime index rebuilds
*   **Comprehensive backups (3-2-1 strategy)** protect against data loss
*   **Monitoring and alerting** catch issues before they impact users
*   **Performance optimization** (caching, batching, tuning) maintains low latency
*   **Cost optimization** balances performance with budget constraints

With this chapter complete, you now have a comprehensive understanding of vector databases and embeddings—from mathematical foundations through production operations. The next chapter will explore advanced RAG techniques that build upon this foundation.
