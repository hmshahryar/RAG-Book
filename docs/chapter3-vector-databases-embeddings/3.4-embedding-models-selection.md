---
id: embedding-models-selection
title: Embedding Models Selection and Fine-tuning
---

## 3.4 Embedding Models Selection and Fine-tuning

Selecting the right embedding model is critical for RAG performance. This section covers how to evaluate, select, and fine-tune embedding models for banking and enterprise applications.

### Embedding Model Selection Criteria

**Key Factors:**

1.  **Domain Alignment:** How well the model understands your industry's language
2.  **Performance:** Retrieval accuracy on your specific use case
3.  **Latency:** Inference speed for real-time applications
4.  **Dimensionality:** Balance between expressiveness and efficiency
5.  **Deployment:** Self-hosted vs. API-based
6.  **Cost:** Infrastructure, API fees, and operational expenses
7.  **Licensing:** Open-source vs. proprietary constraints

### Popular Embedding Models

#### General-Purpose Models

**1. Sentence Transformers (Open-Source)**

| Model | Dimensions | Speed | Quality | Use Case |
|-------|------------|-------|---------|----------|
| `all-MiniLM-L6-v2` | 384 | Very Fast | Good | High-throughput, simple queries |
| `all-mpnet-base-v2` | 768 | Fast | Excellent | General-purpose RAG |
| `all-MiniLM-L12-v2` | 384 | Fast | Very Good | Balanced performance |

**2. BGE (BAAI General Embedding)**

| Model | Dimensions | Speed | Quality | Use Case |
|-------|------------|-------|---------|----------|
| `BAAI/bge-small-en-v1.5` | 384 | Very Fast | Very Good | Resource-constrained |
| `BAAI/bge-base-en-v1.5` | 768 | Fast | Excellent | General-purpose |
| `BAAI/bge-large-en-v1.5` | 1024 | Medium | Outstanding | High-accuracy needs |

**3. OpenAI Embeddings (API-Based)**

| Model | Dimensions | Cost | Quality | Use Case |
|-------|------------|------|---------|----------|
| `text-embedding-3-small` | 512-1536 | Low | Excellent | Cost-sensitive |
| `text-embedding-3-large` | 256-3072 | Medium | Outstanding | Maximum quality |
| `text-embedding-ada-002` | 1536 | Low | Very Good | Legacy applications |

**Industrial-Grade Aspect:** For banking, start with `all-mpnet-base-v2` or `bge-base-en-v1.5` for self-hosted deployments. These provide excellent quality while maintaining data sovereignty.

#### Domain-Specific Models

**Financial Services:**
*   **FinBERT:** Fine-tuned on financial news and reports
*   **SecBERT:** Trained on SEC filings
*   **Use Case:** Improved understanding of financial terminology, ratios, and market concepts

**Legal:**
*   **Legal-BERT:** Fine-tuned on legal documents
*   **CaseLaw-BERT:** Trained on court cases
*   **Use Case:** Better handling of legal terminology and citations

**Healthcare:**
*   **BioBERT:** Trained on biomedical literature
*   **PubMedBERT:** Optimized for PubMed articles
*   **Use Case:** Medical terminology and clinical concepts

**Industrial-Grade Aspect:** For banking compliance and regulatory documents, consider fine-tuning a general model on your specific corpus (Basel III documents, Dodd-Frank regulations, internal policies).

### Benchmarking Embedding Models

#### Using MTEB (Massive Text Embedding Benchmark)

**Categories:**
1.  **Retrieval:** How well the model finds relevant documents
2.  **Classification:** Text categorization accuracy
3.  **Clustering:** Semantic grouping quality
4.  **Semantic Textual Similarity (STS):** Measuring similarity between texts
5.  **Reranking:** Improving initial retrieval results

**Example Benchmark Results (MTEB Retrieval):**
*   `bge-large-en-v1.5`: 54.1% (state-of-the-art open-source)
*   `text-embedding-3-large`: 55.0% (best overall)
*   `all-mpnet-base-v2`: 43.8% (solid general-purpose)

**Industrial-Grade Aspect:** Public benchmarks are useful but don't replace domain-specific evaluation. Create a custom test set with 100-500 real banking queries and relevant documents.

#### Creating Custom Evaluation Datasets

**Process:**
1.  **Collect Queries:** Gather real user questions or create synthetic ones
2.  **Label Relevance:** For each query, identify relevant documents (ground truth)
3.  **Compute Metrics:**
    *   **Recall@K:** Percentage of relevant docs in top K results
    *   **MRR (Mean Reciprocal Rank):** Average of 1/rank of first relevant result
    *   **NDCG@K:** Normalized Discounted Cumulative Gain (accounts for ranking quality)

**Example Evaluation Code:**
```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

# Load model
model = SentenceTransformer('all-mpnet-base-v2')

# Encode corpus and queries
corpus_embeddings = model.encode(corpus, convert_to_tensor=True)
query_embeddings = model.encode(queries, convert_to_tensor=True)

# Compute similarities
similarities = util.cos_sim(query_embeddings, corpus_embeddings)

# Calculate Recall@10
def recall_at_k(similarities, ground_truth, k=10):
    recalls = []
    for i, sim in enumerate(similarities):
        top_k = sim.argsort(descending=True)[:k]
        relevant = ground_truth[i]
        found = len(set(top_k.tolist()) & set(relevant))
        recalls.append(found / len(relevant))
    return np.mean(recalls)

recall_10 = recall_at_k(similarities, ground_truth_labels, k=10)
print(f"Recall@10: {recall_10:.2%}")
```

### Fine-Tuning Embedding Models

When general-purpose models underperform on your domain, fine-tuning can significantly improve results.

#### When to Fine-Tune

**Fine-tune if:**
*   Domain-specific terminology is poorly understood
*   Retrieval accuracy is below acceptable thresholds (\<80% Recall@10)
*   You have sufficient labeled data (1000+ query-document pairs)
*   Data sovereignty allows self-hosted models

**Don't fine-tune if:**
*   General models already perform well (>90% Recall@10)
*   You lack labeled training data
*   You use API-based models (OpenAI, Cohere—not fine-tunable)

#### Fine-Tuning Approaches

**1. Contrastive Learning (Most Common)**

**Method:** Train the model to bring similar pairs closer and push dissimilar pairs apart.

**Data Format:**
```json
[
  {
    "query": "What are Basel III capital requirements?",
    "positive": "Basel III requires banks to maintain a minimum CET1 ratio of 4.5%...",
    "negative": "The Federal Reserve conducts stress tests annually..."
  }
]
```

**Training Code (Sentence Transformers):**
```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# Load base model
model = SentenceTransformer('all-mpnet-base-v2')

# Prepare training data
train_examples = [
    InputExample(texts=[query, positive_doc], label=1.0),
    InputExample(texts=[query, negative_doc], label=0.0)
]

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Define loss function
train_loss = losses.CosineSimilarityLoss(model)

# Fine-tune
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=100,
    output_path='./fine-tuned-model'
)
```

**2. Multiple Negatives Ranking Loss**

**Method:** For each query, provide one positive and multiple negative documents. The model learns to rank the positive highest.

**Advantages:**
*   More efficient than pairwise contrastive learning
*   Better handles hard negatives (similar but irrelevant documents)

**3. Knowledge Distillation**

**Method:** Use a larger, more powerful model (teacher) to train a smaller, faster model (student).

**Use Case:** Create a lightweight model for edge deployment while maintaining quality.

#### Generating Training Data

**1. Manual Labeling:**
*   Subject matter experts label query-document pairs
*   High quality but expensive and slow
*   Recommended for critical banking applications

**2. Synthetic Data Generation:**
*   Use LLMs to generate queries from documents
*   Fast and scalable but potentially noisy

**Example (Using GPT-4):**
```python
from openai import OpenAI

client = OpenAI()

def generate_queries(document_chunk):
    prompt = f"""Generate 3 diverse questions that this document chunk would answer:

Document: {document_chunk}

Questions:"""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.split('\n')
```

**3. Mining Hard Negatives:**
*   Use current model to retrieve top results
*   Label results as positive or negative
*   Use false positives as hard negatives for training

**Industrial-Grade Aspect:** For banking, combine approaches—use SMEs for critical regulatory queries (high quality) and synthetic generation for general knowledge base (scale).

### Model Deployment Strategies

#### Self-Hosted Deployment

**Option 1: Hugging Face Transformers**
```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')
model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')

def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # Mean pooling
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.numpy()
```

**Option 2: Sentence Transformers (Simpler)**
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-mpnet-base-v2')
embeddings = model.encode(["Text to embed"], convert_to_numpy=True)
```

**Option 3: Optimized Inference (ONNX, TensorRT)**
```python
from optimum.onnxruntime import ORTModelForFeatureExtraction

model = ORTModelForFeatureExtraction.from_pretrained(
    'sentence-transformers/all-mpnet-base-v2',
    export=True
)
# 2-3x faster inference
```

#### API-Based Deployment

**OpenAI:**
```python
from openai import OpenAI

client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-large",
    input="Text to embed"
)
embedding = response.data[0].embedding
```

**Cohere:**
```python
import cohere

co = cohere.Client(api_key="YOUR_API_KEY")

response = co.embed(
    texts=["Text to embed"],
    model="embed-english-v3.0",
    input_type="search_document"
)
embeddings = response.embeddings
```

**Industrial-Grade Aspect:** For banking, self-hosted deployment is preferred for data sovereignty. Use GPU instances (AWS g4dn, g5) for production to achieve \<50ms embedding latency.

### Embedding Model Versioning and Management

**Best Practices:**

1.  **Version Control:**
    *   Tag models with semantic versioning (v1.0.0, v1.1.0)
    *   Store model artifacts in version-controlled storage (S3, GCS)

2.  **A/B Testing:**
    *   Deploy new models alongside existing ones
    *   Route percentage of traffic to new model
    *   Compare retrieval quality metrics

3.  **Rollback Strategy:**
    *   Maintain previous model version for quick rollback
    *   Monitor error rates and latency after deployment

4.  **Re-indexing Plan:**
    *   Changing embedding models requires re-indexing entire corpus
    *   Plan for downtime or dual-index strategy
    *   Estimate costs (compute, storage, time)

**Industrial-Grade Aspect:** For banks, establish a formal change management process for embedding model updates. Document model versions, training data, and performance metrics for audit compliance.

### Summary

Embedding model selection and fine-tuning are critical for RAG success. Key takeaways:

*   **Start with general-purpose models** (`all-mpnet-base-v2`, `bge-base-en-v1.5`)
*   **Benchmark on domain-specific data** to validate performance
*   **Fine-tune when necessary** using contrastive learning or distillation
*   **Self-hosted models** provide data sovereignty for banking
*   **Version control and A/B testing** ensure safe model updates
*   **Re-indexing is expensive**—plan model changes carefully

In the next section, we'll explore metadata management and hybrid search strategies.
