---
id: query-decomposition
title: Query Decomposition and Multi-hop Reasoning
---

## 4.1 Query Decomposition and Multi-hop Reasoning

Complex queries often require breaking down into simpler sub-queries and reasoning across multiple retrieval steps. This section explores query decomposition strategies and multi-hop reasoning patterns for advanced RAG systems.

### The Challenge of Complex Queries

**Simple Query:** "What is the Basel III capital requirement?"
- Single retrieval step
- Direct answer in one document

**Complex Query:** "How do Basel III capital requirements differ from Basel II, and what impact did these changes have on US banks' lending practices between 2013-2020?"
- Multiple concepts (Basel III vs Basel II, capital requirements, lending impact)
- Temporal reasoning (2013-2020)
- Causal relationships (requirements â†’ lending practices)
- Requires multiple documents and synthesis

**Industrial-Grade Aspect:** Banking queries are often complex, involving regulatory comparisons, temporal analysis, and cross-document reasoning.

### Query Decomposition Strategies

#### 1. LLM-Based Decomposition

**Approach:** Use an LLM to break complex queries into simpler sub-queries.

```python
from openai import OpenAI

client = OpenAI()

def decompose_query(complex_query):
    """Decompose complex query into sub-queries."""
    prompt = f"""Break down this complex question into 2-4 simpler sub-questions that, when answered together, would address the original question.

Original Question: {complex_query}

Sub-questions (numbered list):"""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    # Parse sub-questions
    sub_queries = [
        line.strip() for line in response.choices[0].message.content.split('\n')
        if line.strip() and line[0].isdigit()
    ]
    
    return sub_queries

# Example
complex_query = "How do Basel III capital requirements differ from Basel II, and what impact did these changes have on US banks' lending practices between 2013-2020?"

sub_queries = decompose_query(complex_query)
# Output:
# 1. What are the Basel III capital requirements?
# 2. What were the Basel II capital requirements?
# 3. What are the key differences between Basel III and Basel II capital requirements?
# 4. How did Basel III implementation affect US banks' lending practices from 2013-2020?
```

#### 2. Template-Based Decomposition

**Approach:** Use predefined patterns for common query types.

```python
import re

def template_decompose(query):
    """Decompose using templates for common patterns."""
    sub_queries = []
    
    # Pattern: "Compare X and Y"
    compare_pattern = r"(?:compare|difference between|differ from)\s+(.+?)\s+(?:and|vs\.?)\s+(.+?)(?:\?|$|,)"
    match = re.search(compare_pattern, query, re.IGNORECASE)
    if match:
        concept_a, concept_b = match.groups()
        sub_queries.extend([
            f"What is {concept_a}?",
            f"What is {concept_b}?",
            f"What are the differences between {concept_a} and {concept_b}?"
        ])
    
    # Pattern: "Impact of X on Y"
    impact_pattern = r"(?:impact|effect) of (.+?) on (.+?)(?:\?|$)"
    match = re.search(impact_pattern, query, re.IGNORECASE)
    if match:
        cause, effect = match.groups()
        sub_queries.extend([
            f"What is {cause}?",
            f"How does {cause} affect {effect}?",
            f"What are examples of {cause} impacting {effect}?"
        ])
    
    return sub_queries if sub_queries else [query]
```

#### 3. Dependency-Aware Decomposition

**Approach:** Identify dependencies between sub-queries and execute in order.

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SubQuery:
    query: str
    dependencies: List[int]  # Indices of queries this depends on
    index: int

def decompose_with_dependencies(complex_query):
    """Decompose query with dependency tracking."""
    # Use LLM to identify dependencies
    prompt = f"""Break down this question into sub-questions. For each sub-question, identify which previous sub-questions (by number) it depends on.

Question: {complex_query}

Format:
1. [Sub-question] (depends on: none)
2. [Sub-question] (depends on: 1)
etc."""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    
    # Parse response into SubQuery objects
    sub_queries = []
    for i, line in enumerate(response.choices[0].message.content.split('\n')):
        if line.strip() and line[0].isdigit():
            # Extract query and dependencies
            query_match = re.search(r'\d+\.\s+(.+?)\s+\(depends on:', line)
            deps_match = re.search(r'\(depends on:\s*(.+?)\)', line)
            
            if query_match:
                query_text = query_match.group(1)
                deps = []
                if deps_match and deps_match.group(1) != 'none':
                    deps = [int(d.strip()) - 1 for d in deps_match.group(1).split(',')]
                
                sub_queries.append(SubQuery(query_text, deps, i))
    
    return sub_queries
```

### Multi-Hop Reasoning Patterns

#### 1. Sequential Retrieval

**Pattern:** Execute sub-queries in sequence, using results from previous steps.

```python
def sequential_rag(sub_queries):
    """Execute RAG for each sub-query sequentially."""
    results = []
    accumulated_context = ""
    
    for sub_query in sub_queries:
        # Retrieve for current sub-query
        retrieved_docs = vector_search(sub_query.query)
        
        # Generate answer using accumulated context
        prompt = f"""Previous context:
{accumulated_context}

New information:
{format_documents(retrieved_docs)}

Question: {sub_query.query}

Answer:"""
        
        answer = llm_generate(prompt)
        results.append(answer)
        
        # Accumulate context for next iteration
        accumulated_context += f"\nQ: {sub_query.query}\nA: {answer}\n"
    
    return results

def synthesize_final_answer(original_query, sub_results):
    """Combine sub-answers into final answer."""
    prompt = f"""Based on the following sub-questions and answers, provide a comprehensive answer to the original question.

Original Question: {original_query}

Sub-questions and Answers:
{format_sub_results(sub_results)}

Comprehensive Answer:"""
    
    return llm_generate(prompt)
```

#### 2. Parallel Retrieval with Fusion

**Pattern:** Execute independent sub-queries in parallel, then fuse results.

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_rag(sub_queries):
    """Execute independent sub-queries in parallel."""
    def process_sub_query(sub_query):
        docs = vector_search(sub_query.query)
        answer = llm_generate(f"Question: {sub_query.query}\n\nContext: {format_documents(docs)}\n\nAnswer:")
        return {"query": sub_query.query, "answer": answer, "docs": docs}
    
    # Identify independent queries (no dependencies)
    independent = [sq for sq in sub_queries if not sq.dependencies]
    
    # Execute in parallel
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(process_sub_query, independent))
    
    return results
```

#### 3. Graph-Based Reasoning

**Pattern:** Model query as a graph of concepts and traverse to find answer.

```python
import networkx as nx

def graph_based_reasoning(query):
    """Use knowledge graph for multi-hop reasoning."""
    # Extract entities from query
    entities = extract_entities(query)
    
    # Build subgraph connecting entities
    G = nx.Graph()
    for entity in entities:
        # Retrieve related concepts
        related = vector_search(f"What is related to {entity}?")
        for rel in related:
            G.add_edge(entity, rel['concept'], weight=rel['score'])
    
    # Find paths between key entities
    start, end = entities[0], entities[-1]
    paths = nx.all_simple_paths(G, start, end, cutoff=3)
    
    # Retrieve documents for each path
    for path in paths:
        path_docs = []
        for i in range(len(path) - 1):
            query = f"How does {path[i]} relate to {path[i+1]}?"
            path_docs.extend(vector_search(query))
        
        # Generate answer from path
        answer = llm_generate(f"Explain the relationship: {' -> '.join(path)}\n\nContext: {format_documents(path_docs)}")
        yield {"path": path, "answer": answer}
```

### Advanced Decomposition Techniques

#### 1. Iterative Refinement

**Approach:** Refine sub-queries based on initial results.

```python
def iterative_decomposition(original_query, max_iterations=3):
    """Iteratively refine decomposition based on results."""
    sub_queries = decompose_query(original_query)
    
    for iteration in range(max_iterations):
        # Execute current sub-queries
        results = [rag_query(sq) for sq in sub_queries]
        
        # Check if answer is complete
        completeness = check_completeness(original_query, results)
        if completeness > 0.9:
            break
        
        # Identify gaps and generate new sub-queries
        gaps = identify_gaps(original_query, results)
        new_sub_queries = [generate_query_for_gap(gap) for gap in gaps]
        
        sub_queries.extend(new_sub_queries)
    
    return synthesize_final_answer(original_query, results)

def check_completeness(original_query, results):
    """Assess if results fully answer the query."""
    prompt = f"""Original question: {original_query}

Current answers: {results}

On a scale of 0-1, how completely do these answers address the original question? Respond with just a number."""
    
    response = llm_generate(prompt)
    return float(response.strip())
```

#### 2. Hypothesis-Driven Decomposition

**Approach:** Generate hypotheses and decompose to verify/refute them.

```python
def hypothesis_driven_rag(query):
    """Generate and test hypotheses."""
    # Generate hypotheses
    hypotheses = generate_hypotheses(query)
    
    results = []
    for hypothesis in hypotheses:
        # Decompose hypothesis into verifiable claims
        claims = decompose_to_claims(hypothesis)
        
        # Verify each claim
        evidence = []
        for claim in claims:
            docs = vector_search(claim)
            verification = verify_claim(claim, docs)
            evidence.append(verification)
        
        # Assess hypothesis
        hypothesis_score = assess_hypothesis(hypothesis, evidence)
        results.append({
            "hypothesis": hypothesis,
            "score": hypothesis_score,
            "evidence": evidence
        })
    
    # Select best-supported hypothesis
    best = max(results, key=lambda x: x['score'])
    return best

def generate_hypotheses(query):
    """Generate possible answers as hypotheses."""
    prompt = f"""Generate 3 possible hypotheses that could answer this question:

Question: {query}

Hypotheses:"""
    
    response = llm_generate(prompt)
    return [h.strip() for h in response.split('\n') if h.strip()]
```

### Industrial Banking Use Cases

#### Use Case 1: Regulatory Compliance Analysis

**Query:** "What are the differences in stress testing requirements between Dodd-Frank and Basel III, and how do they affect capital planning for systemically important banks?"

**Decomposition:**
1. What are Dodd-Frank stress testing requirements?
2. What are Basel III stress testing requirements?
3. What are the key differences between these requirements?
4. How do these requirements impact capital planning?
5. What additional considerations apply to systemically important banks?

#### Use Case 2: Risk Assessment

**Query:** "How does the correlation between interest rate changes and credit default rates vary across different loan portfolios, and what does this mean for our risk-weighted assets calculation?"

**Decomposition:**
1. What is the historical correlation between interest rates and credit defaults?
2. How does this correlation differ across loan types (mortgage, commercial, consumer)?
3. What is the current methodology for calculating risk-weighted assets?
4. How should correlation findings inform RWA calculations?

**Industrial-Grade Aspect:** Multi-hop reasoning is essential for banking compliance queries that require synthesizing information from multiple regulatory sources and applying it to specific scenarios.

### Best Practices

1. **Limit Decomposition Depth:** 2-4 sub-queries optimal; more leads to error accumulation
2. **Validate Sub-Queries:** Ensure sub-queries are answerable and relevant
3. **Track Provenance:** Maintain links between sub-answers and source documents
4. **Handle Failures:** Implement fallbacks if sub-query fails
5. **Cost Management:** Each sub-query incurs LLM costs; balance thoroughness with budget

### Summary

Query decomposition and multi-hop reasoning enable RAG systems to handle complex queries. Key takeaways:

- **Decomposition strategies**: LLM-based, template-based, dependency-aware
- **Multi-hop patterns**: Sequential, parallel, graph-based
- **Advanced techniques**: Iterative refinement, hypothesis-driven
- **Banking applications**: Regulatory analysis, risk assessment, compliance
- **Best practices**: Limit depth, validate queries, track provenance

Next, we'll explore contextual compression and reranking to improve retrieval quality.
