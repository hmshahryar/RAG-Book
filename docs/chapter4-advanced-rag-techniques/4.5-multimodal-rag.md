---
id: multimodal-rag
title: Multi-modal RAG (Text, Images, Tables)
---

## 4.5 Multi-modal RAG (Text, Images, Tables)

Banking documents contain more than just textâ€”charts, tables, diagrams, and images convey critical information. Multi-modal RAG extends traditional text-based RAG to handle diverse content types.

### The Multi-modal Challenge

**Banking Document Types:**
- **Financial Reports:** Tables of financial data, charts, graphs
- **Regulatory Filings:** Forms with structured data
- **Compliance Documents:** Flowcharts, decision trees
- **Risk Assessments:** Heat maps, scatter plots
- **Presentations:** Slides with mixed content

**Traditional RAG Limitations:**
- Text-only embeddings miss visual information
- Tables converted to text lose structure
- Charts and graphs are ignored or poorly described

### Multi-modal Embedding Models

#### 1. CLIP (Contrastive Language-Image Pre-training)

**Capability:** Joint text-image embeddings

```python
from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def embed_image(image):
    """Generate embedding for image."""
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
    return image_features.numpy()

def embed_text(text):
    """Generate embedding for text (same space as images)."""
    inputs = processor(text=[text], return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs)
    return text_features.numpy()

# Search images with text query
query_embedding = embed_text("capital adequacy ratio chart")
image_results = vector_search(query_embedding, collection="financial_charts")
```

#### 2. Multi-modal LLMs (GPT-4V, Claude 3, Gemini)

**Capability:** Process images and text together

```python
from openai import OpenAI

client = OpenAI()

def multimodal_rag(query, include_images=True):
    """RAG with image understanding."""
    
    # Retrieve relevant documents (text + images)
    text_docs = vector_search(query, collection="text_docs", top_k=3)
    
    if include_images:
        image_docs = vector_search(query, collection="images", top_k=2)
    else:
        image_docs = []
    
    # Build multi-modal prompt
    messages = [
        {"role": "user", "content": [
            {"type": "text", "text": f"Question: {query}\n\nContext:"},
        ]}
    ]
    
    # Add text context
    for doc in text_docs:
        messages[0]["content"].append({
            "type": "text",
            "text": f"\n{doc['text']}"
        })
    
    # Add images
    for img_doc in image_docs:
        messages[0]["content"].append({
            "type": "image_url",
            "image_url": {"url": img_doc['url']}
        })
    
    messages[0]["content"].append({
        "type": "text",
        "text": "\nAnswer:"
    })
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=messages
    )
    
    return response.choices[0].message.content
```

### Table Understanding

#### 1. Table Extraction and Structuring

```python
import pandas as pd
from tabula import read_pdf

def extract_tables_from_pdf(pdf_path):
    """Extract tables from PDF."""
    tables = read_pdf(pdf_path, pages='all', multiple_tables=True)
    return tables

def table_to_text(df):
    """Convert table to text representation."""
    # Option 1: Markdown format
    markdown = df.to_markdown(index=False)
    
    # Option 2: Natural language description
    description = f"Table with {len(df)} rows and {len(df.columns)} columns. "
    description += f"Columns: {', '.join(df.columns)}. "
    
    # Add summary statistics
    for col in df.select_dtypes(include=['number']).columns:
        description += f"{col} ranges from {df[col].min()} to {df[col].max()}. "
    
    return markdown, description
```

#### 2. Table-Specific Embeddings

```python
from transformers import TapasTokenizer, TapasForQuestionAnswering

tokenizer = TapasTokenizer.from_pretrained("google/tapas-base-finetuned-wtq")
model = TapasForQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

def query_table(table_df, query):
    """Answer questions about tables."""
    inputs = tokenizer(
        table=table_df,
        queries=[query],
        padding="max_length",
        return_tensors="pt"
    )
    
    outputs = model(**inputs)
    predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(
        inputs,
        outputs.logits.detach(),
        outputs.logits_aggregation.detach()
    )
    
    # Extract answer from table
    answers = []
    for coordinates in predicted_answer_coordinates[0]:
        if coordinates:
            cell_values = [table_df.iat[coord] for coord in coordinates]
            answers.append(", ".join(map(str, cell_values)))
    
    return answers[0] if answers else "No answer found"
```

#### 3. Hybrid Table RAG

```python
def table_aware_rag(query):
    """RAG that handles both text and tables."""
    
    # Retrieve text documents
    text_docs = vector_search(query, collection="text", top_k=3)
    
    # Retrieve tables
    table_docs = vector_search(query, collection="tables", top_k=2)
    
    # Process tables
    table_answers = []
    for table_doc in table_docs:
        table_df = pd.read_json(table_doc['data'])
        answer = query_table(table_df, query)
        table_answers.append({
            "table_id": table_doc['id'],
            "answer": answer,
            "table": table_df.to_markdown()
        })
    
    # Combine context
    prompt = f"""Question: {query}

Text Context:
{format_documents(text_docs)}

Table Data:
{format_table_answers(table_answers)}

Answer:"""
    
    return llm_generate(prompt)
```

### Document Layout Understanding

#### 1. Layout-Aware Parsing

```python
from unstructured.partition.auto import partition

def parse_complex_document(file_path):
    """Parse document preserving layout."""
    elements = partition(filename=file_path)
    
    structured_content = {
        "text": [],
        "tables": [],
        "images": [],
        "metadata": []
    }
    
    for element in elements:
        if element.category == "Table":
            structured_content["tables"].append(element.metadata.text_as_html)
        elif element.category == "Image":
            structured_content["images"].append(element.metadata.image_path)
        elif element.category in ["Title", "NarrativeText"]:
            structured_content["text"].append(str(element))
    
    return structured_content
```

#### 2. Visual Document Understanding

```python
from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering

processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base")
model = LayoutLMv3ForQuestionAnswering.from_pretrained("microsoft/layoutlmv3-base")

def visual_document_qa(image, question):
    """Answer questions about document images."""
    encoding = processor(image, question, return_tensors="pt")
    outputs = model(**encoding)
    
    # Get answer span
    answer_start = outputs.start_logits.argmax()
    answer_end = outputs.end_logits.argmax()
    
    answer = processor.tokenizer.decode(
        encoding.input_ids[0][answer_start:answer_end+1]
    )
    
    return answer
```

### Banking Use Cases

#### Use Case 1: Financial Statement Analysis

```python
def analyze_financial_statement(statement_pdf, query):
    """Analyze financial statements with tables and charts."""
    
    # Extract components
    content = parse_complex_document(statement_pdf)
    
    # Process tables (balance sheet, income statement)
    financial_data = {}
    for table_html in content['tables']:
        df = pd.read_html(table_html)[0]
        financial_data[identify_table_type(df)] = df
    
    # Process charts (if any)
    chart_descriptions = []
    for img_path in content['images']:
        description = describe_chart(img_path)
        chart_descriptions.append(description)
    
    # Answer query
    if "ratio" in query.lower():
        # Calculate from tables
        ratio = calculate_ratio_from_tables(query, financial_data)
        return f"Based on the financial statements: {ratio}"
    else:
        # General RAG
        context = format_financial_context(content, financial_data, chart_descriptions)
        return llm_generate_with_context(query, context)
```

#### Use Case 2: Regulatory Form Processing

```python
def process_regulatory_form(form_pdf, validation_rules):
    """Process structured regulatory forms."""
    
    # Extract form fields
    form_data = extract_form_fields(form_pdf)
    
    # Validate against rules
    validation_results = []
    for rule in validation_rules:
        result = validate_field(form_data, rule)
        validation_results.append(result)
    
    # Generate compliance report
    report = generate_compliance_report(form_data, validation_results)
    
    return report
```

### Best Practices

1. **Separate Collections:** Store text, images, and tables in separate vector collections
2. **Rich Metadata:** Include content type, source page, bounding boxes
3. **Hybrid Retrieval:** Combine text and visual search
4. **Table Normalization:** Standardize table formats for consistent processing
5. **Image Descriptions:** Generate text descriptions of images for fallback search

### Summary

Multi-modal RAG handles diverse content types in banking documents. Key takeaways:

- **CLIP embeddings** enable text-image search
- **Multi-modal LLMs** (GPT-4V) process images and text together
- **Table-specific models** (TAPAS, LayoutLM) understand structured data
- **Layout-aware parsing** preserves document structure
- **Banking applications**: Financial statement analysis, form processing

Next, we'll explore temporal RAG for version control and time-based queries.
