---
id: self-rag-corrective-rag
title: Self-RAG and Corrective RAG
---

## 4.3 Self-RAG and Corrective RAG

Self-RAG and Corrective RAG are advanced patterns that enable RAG systems to self-assess and self-correct, improving answer quality through reflection and iterative refinement.

### Self-RAG: Self-Reflective Retrieval-Augmented Generation

**Concept:** The LLM decides when to retrieve, what to retrieve, and whether retrieved information is useful—then generates and critiques its own output.

**Key Components:**
1. **Retrieval Decision:** Should I retrieve documents for this query?
2. **Relevance Assessment:** Are retrieved documents relevant?
3. **Support Evaluation:** Does my answer use the retrieved information?
4. **Quality Critique:** Is my answer accurate and complete?

#### Implementation

```python
def self_rag(query):
    """Self-RAG with reflection tokens."""
    
    # Step 1: Decide if retrieval is needed
    retrieval_decision = llm_generate(f"""Should retrieval be performed for this query?
Query: {query}
Answer with 'yes' or 'no':""")
    
    if 'yes' in retrieval_decision.lower():
        # Step 2: Retrieve documents
        documents = vector_search(query, top_k=5)
        
        # Step 3: Assess relevance of each document
        relevant_docs = []
        for doc in documents:
            relevance = llm_generate(f"""Is this document relevant to the query?
Query: {query}
Document: {doc['text'][:300]}...
Relevance (relevant/irrelevant):""")
            
            if 'relevant' in relevance.lower():
                relevant_docs.append(doc)
        
        context = format_documents(relevant_docs)
    else:
        context = ""
    
    # Step 4: Generate answer
    answer = llm_generate(f"""Query: {query}
Context: {context}
Answer:""")
    
    # Step 5: Critique answer
    critique = llm_generate(f"""Evaluate this answer:
Query: {query}
Answer: {answer}
Context: {context}

Is the answer:
1. Supported by the context? (yes/no)
2. Accurate? (yes/no)
3. Complete? (yes/no)

Evaluation:""")
    
    # Step 6: Refine if needed
    if 'no' in critique.lower():
        refined_answer = llm_generate(f"""Improve this answer based on the critique:
Original answer: {answer}
Critique: {critique}
Context: {context}

Improved answer:""")
        return refined_answer
    
    return answer
```

**Industrial-Grade Aspect:** For banking, self-assessment reduces hallucinations in regulatory responses—the system won't confidently state incorrect capital requirements.

### Corrective RAG (CRAG)

**Concept:** Automatically detect and correct retrieval errors through iterative refinement.

**Workflow:**
1. Initial retrieval
2. Assess retrieval quality
3. If poor quality → trigger corrective actions:
   - Web search for additional context
   - Query reformulation
   - Decomposition into sub-queries
4. Re-retrieve and re-generate

#### Implementation

```python
def corrective_rag(query, quality_threshold=0.7):
    """CRAG with automatic correction."""
    
    # Initial retrieval
    documents = vector_search(query, top_k=5)
    
    # Assess retrieval quality
    quality_score = assess_retrieval_quality(query, documents)
    
    if quality_score < quality_threshold:
        # Trigger corrective actions
        corrected_docs = apply_corrections(query, documents, quality_score)
    else:
        corrected_docs = documents
    
    # Generate answer
    answer = llm_generate_with_context(query, corrected_docs)
    return answer

def assess_retrieval_quality(query, documents):
    """Score retrieval quality (0-1)."""
    prompt = f"""Rate the quality of these retrieved documents for answering the query.

Query: {query}

Documents:
{format_documents(documents)}

Quality score (0.0-1.0):"""
    
    response = llm_generate(prompt)
    try:
        return float(response.strip())
    except:
        return 0.5

def apply_corrections(query, documents, quality_score):
    """Apply corrective strategies based on quality."""
    
    if quality_score < 0.3:
        # Very poor retrieval → try web search
        web_results = web_search(query)
        return documents + web_results
    
    elif quality_score < 0.5:
        # Moderate quality → reformulate query
        reformulated = reformulate_query(query)
        new_docs = vector_search(reformulated, top_k=5)
        return new_docs
    
    else:
        # Decent quality → expand with related queries
        related_queries = generate_related_queries(query)
        expanded_docs = documents.copy()
        for rq in related_queries[:2]:
            expanded_docs.extend(vector_search(rq, top_k=2))
        return expanded_docs
```

### Adaptive Retrieval

**Concept:** Dynamically adjust retrieval strategy based on query characteristics.

```python
def adaptive_rag(query):
    """Adapt retrieval strategy to query type."""
    
    # Classify query
    query_type = classify_query_type(query)
    
    if query_type == "factual":
        # Simple factual query → standard RAG
        docs = vector_search(query, top_k=3)
        return generate_answer(query, docs)
    
    elif query_type == "complex":
        # Complex query → decomposition + multi-hop
        sub_queries = decompose_query(query)
        results = [rag_query(sq) for sq in sub_queries]
        return synthesize_answer(query, results)
    
    elif query_type == "comparative":
        # Comparison → retrieve for each entity separately
        entities = extract_entities(query)
        entity_docs = {e: vector_search(e, top_k=3) for e in entities}
        return generate_comparative_answer(query, entity_docs)
    
    elif query_type == "temporal":
        # Time-based query → filter by date range
        date_range = extract_date_range(query)
        docs = vector_search_with_filter(query, date_filter=date_range, top_k=5)
        return generate_answer(query, docs)

def classify_query_type(query):
    """Classify query into types."""
    prompt = f"""Classify this query into one of: factual, complex, comparative, temporal

Query: {query}

Type:"""
    return llm_generate(prompt).strip().lower()
```

### Confidence-Based Retrieval

**Approach:** Retrieve more documents when confidence is low.

```python
def confidence_based_rag(query, initial_k=3, max_k=10):
    """Retrieve more documents if initial answer has low confidence."""
    
    # Initial retrieval and generation
    docs = vector_search(query, top_k=initial_k)
    answer = llm_generate_with_context(query, docs)
    
    # Assess confidence
    confidence = assess_answer_confidence(query, answer, docs)
    
    # If low confidence, retrieve more
    if confidence < 0.7 and initial_k < max_k:
        additional_docs = vector_search(query, top_k=max_k)[initial_k:]
        all_docs = docs + additional_docs
        
        # Re-generate with more context
        answer = llm_generate_with_context(query, all_docs)
        confidence = assess_answer_confidence(query, answer, all_docs)
    
    return {
        "answer": answer,
        "confidence": confidence,
        "num_docs_used": len(docs)
    }

def assess_answer_confidence(query, answer, documents):
    """Estimate confidence in answer (0-1)."""
    prompt = f"""Rate your confidence in this answer on a scale of 0-1.

Query: {query}
Answer: {answer}
Supporting documents: {len(documents)}

Confidence (0.0-1.0):"""
    
    response = llm_generate(prompt)
    try:
        return float(response.strip())
    except:
        return 0.5
```

### Iterative Refinement

**Approach:** Iteratively improve answer through multiple retrieval-generation cycles.

```python
def iterative_rag(query, max_iterations=3):
    """Iteratively refine answer."""
    answer = ""
    context_docs = []
    
    for iteration in range(max_iterations):
        # Identify gaps in current answer
        if iteration == 0:
            gaps = [query]  # Start with original query
        else:
            gaps = identify_answer_gaps(query, answer, context_docs)
        
        if not gaps:
            break  # Answer is complete
        
        # Retrieve for gaps
        for gap in gaps:
            new_docs = vector_search(gap, top_k=3)
            context_docs.extend(new_docs)
        
        # Generate improved answer
        answer = llm_generate(f"""Query: {query}

Context:
{format_documents(context_docs)}

Provide a comprehensive answer:""")
    
    return answer

def identify_answer_gaps(query, current_answer, documents):
    """Find what's missing from the current answer."""
    prompt = f"""What information is missing from this answer?

Query: {query}
Current answer: {current_answer}

List 1-3 specific gaps or follow-up questions:"""
    
    response = llm_generate(prompt)
    gaps = [line.strip() for line in response.split('\n') if line.strip()]
    return gaps[:3]  # Limit to 3 gaps
```

### Fact Verification

**Approach:** Verify factual claims in generated answers.

```python
def rag_with_verification(query):
    """RAG with automatic fact checking."""
    
    # Standard RAG
    documents = vector_search(query, top_k=5)
    answer = llm_generate_with_context(query, documents)
    
    # Extract claims from answer
    claims = extract_claims(answer)
    
    # Verify each claim
    verification_results = []
    for claim in claims:
        verified = verify_claim(claim, documents)
        verification_results.append({
            "claim": claim,
            "verified": verified['supported'],
            "evidence": verified['evidence']
        })
    
    # If any claims unverified, flag or refine
    unverified = [v for v in verification_results if not v['verified']]
    
    if unverified:
        # Refine answer to remove unverified claims
        refined_answer = llm_generate(f"""Revise this answer to remove unverified claims:

Original answer: {answer}

Unverified claims:
{format_unverified(unverified)}

Revised answer (only verified information):""")
        
        return {
            "answer": refined_answer,
            "verification_status": "revised",
            "unverified_claims": unverified
        }
    
    return {
        "answer": answer,
        "verification_status": "verified",
        "all_claims_verified": True
    }

def extract_claims(answer):
    """Extract factual claims from answer."""
    prompt = f"""Extract specific factual claims from this answer:

Answer: {answer}

Claims (one per line):"""
    
    response = llm_generate(prompt)
    return [line.strip() for line in response.split('\n') if line.strip()]

def verify_claim(claim, documents):
    """Check if claim is supported by documents."""
    context = format_documents(documents)
    
    prompt = f"""Is this claim supported by the context?

Claim: {claim}

Context: {context}

Answer 'supported' or 'unsupported' and provide evidence:"""
    
    response = llm_generate(prompt)
    return {
        "supported": 'supported' in response.lower(),
        "evidence": response
    }
```

**Industrial-Grade Aspect:** For banking, fact verification is critical—incorrect capital ratios or compliance deadlines can have serious consequences.

### Best Practices

1. **Balance Iterations:** 2-3 iterations optimal; more leads to diminishing returns
2. **Set Quality Thresholds:** Define acceptable confidence/quality scores
3. **Monitor Costs:** Each iteration adds LLM calls
4. **Cache Results:** Avoid re-retrieving for similar queries
5. **Human-in-the-Loop:** For critical banking queries, flag low-confidence answers for review

### Summary

Self-RAG and Corrective RAG enable self-improving systems. Key takeaways:

- **Self-RAG**: LLM decides when to retrieve and critiques its own output
- **Corrective RAG**: Automatically detects and fixes retrieval errors
- **Adaptive retrieval**: Adjusts strategy based on query type
- **Iterative refinement**: Progressively improves answers
- **Fact verification**: Ensures accuracy of generated claims
- **Banking applications**: Reduces hallucinations in regulatory responses

Next, we'll explore Agentic RAG with tool integration.
