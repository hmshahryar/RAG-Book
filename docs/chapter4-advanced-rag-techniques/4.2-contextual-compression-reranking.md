---
id: contextual-compression-reranking
title: Contextual Compression and Reranking
---

## 4.2 Contextual Compression and Reranking

Retrieval often returns verbose or partially relevant documents. Contextual compression and reranking refine retrieved content to improve LLM generation quality while managing token limits. This section explores techniques to optimize the context sent to LLMs.

### The Context Quality Problem

**Challenge:** Initial retrieval (top-K vector search) often includes:
- **Irrelevant sections** within relevant documents
- **Redundant information** across multiple chunks
- **Verbose content** that wastes tokens
- **Suboptimal ranking** (semantic similarity ≠ relevance for specific query)

**Impact:**
- Wasted tokens → higher costs
- Noise in context → lower answer quality
- Exceeding context windows → truncated information

**Industrial-Grade Aspect:** For banking, where regulatory documents are lengthy and dense, compression and reranking are essential for cost-effective, high-quality RAG.

### Reranking Strategies

#### 1. Cross-Encoder Reranking

**Concept:** Unlike bi-encoders (separate query and document embeddings), cross-encoders process query and document together for more accurate relevance scoring.

**Architecture:**
```
Bi-Encoder (Vector Search):
Query → Encoder → Embedding
Document → Encoder → Embedding
Similarity = cosine(query_emb, doc_emb)

Cross-Encoder (Reranking):
[Query, Document] → Encoder → Relevance Score (0-1)
```

**Implementation:**
```python
from sentence_transformers import CrossEncoder

# Load cross-encoder model
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_documents(query, documents, top_k=5):
    """Rerank documents using cross-encoder."""
    # Create query-document pairs
    pairs = [[query, doc['text']] for doc in documents]
    
    # Score all pairs
    scores = reranker.predict(pairs)
    
    # Sort by score and return top-k
    ranked_docs = sorted(
        zip(documents, scores),
        key=lambda x: x[1],
        reverse=True
    )[:top_k]
    
    return [doc for doc, score in ranked_docs]

# Usage
initial_results = vector_search(query, top_k=20)  # Retrieve 20
reranked_results = rerank_documents(query, initial_results, top_k=5)  # Rerank to 5
```

**Models:**
- `cross-encoder/ms-marco-MiniLM-L-6-v2`: Fast, general-purpose
- `cross-encoder/ms-marco-MiniLM-L-12-v2`: Higher quality
- `BAAI/bge-reranker-large`: State-of-the-art

**Industrial-Grade Aspect:** For banking, reranking improves precision—ensuring the top 5 results are truly the most relevant regulatory sections, not just semantically similar.

#### 2. LLM-Based Reranking

**Approach:** Use an LLM to assess relevance and rerank.

```python
def llm_rerank(query, documents, top_k=5):
    """Rerank using LLM relevance assessment."""
    scores = []
    
    for doc in documents:
        prompt = f"""Rate the relevance of this document to the query on a scale of 0-10.

Query: {query}

Document: {doc['text'][:500]}...

Relevance score (0-10):"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            score = float(response.choices[0].message.content.strip())
        except:
            score = 0
        
        scores.append((doc, score))
    
    # Sort and return top-k
    ranked = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]
    return [doc for doc, score in ranked]
```

**Trade-offs:**
- **Pros:** Very accurate, can handle nuanced relevance
- **Cons:** Expensive (LLM call per document), slower

**Use Case:** Critical queries where accuracy justifies cost (e.g., regulatory compliance questions).

#### 3. Hybrid Reranking

**Approach:** Combine multiple signals for ranking.

```python
def hybrid_rerank(query, documents, weights={'vector': 0.4, 'cross_encoder': 0.4, 'bm25': 0.2}):
    """Combine multiple ranking signals."""
    # Get scores from different methods
    vector_scores = {doc['id']: doc['vector_score'] for doc in documents}
    
    # Cross-encoder scores
    ce_scores = dict(zip(
        [doc['id'] for doc in documents],
        reranker.predict([[query, doc['text']] for doc in documents])
    ))
    
    # BM25 scores (keyword matching)
    bm25_scores = compute_bm25_scores(query, documents)
    
    # Normalize scores to 0-1
    def normalize(scores):
        min_s, max_s = min(scores.values()), max(scores.values())
        return {k: (v - min_s) / (max_s - min_s) if max_s > min_s else 0.5 
                for k, v in scores.items()}
    
    vector_norm = normalize(vector_scores)
    ce_norm = normalize(ce_scores)
    bm25_norm = normalize(bm25_scores)
    
    # Weighted combination
    final_scores = {}
    for doc_id in vector_scores.keys():
        final_scores[doc_id] = (
            weights['vector'] * vector_norm[doc_id] +
            weights['cross_encoder'] * ce_norm[doc_id] +
            weights['bm25'] * bm25_norm[doc_id]
        )
    
    # Sort and return
    ranked_ids = sorted(final_scores.keys(), key=lambda x: final_scores[x], reverse=True)
    return [doc for doc in documents if doc['id'] in ranked_ids[:5]]
```

### Contextual Compression Techniques

#### 1. Extractive Compression

**Approach:** Extract only the most relevant sentences from each document.

```python
from transformers import pipeline

# Load extractive QA model
qa_model = pipeline("question-answering", model="deepset/roberta-base-squad2")

def extractive_compress(query, document, max_sentences=3):
    """Extract most relevant sentences from document."""
    sentences = document.split('. ')
    
    # Score each sentence
    sentence_scores = []
    for sent in sentences:
        if len(sent) < 20:  # Skip very short sentences
            continue
        
        try:
            result = qa_model(question=query, context=sent)
            sentence_scores.append((sent, result['score']))
        except:
            sentence_scores.append((sent, 0))
    
    # Return top sentences
    top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:max_sentences]
    return '. '.join([sent for sent, score in top_sentences]) + '.'
```

#### 2. Abstractive Compression

**Approach:** Use an LLM to summarize documents while preserving query-relevant information.

```python
def abstractive_compress(query, document, max_tokens=150):
    """Compress document using abstractive summarization."""
    prompt = f"""Summarize the following document, focusing on information relevant to this query. Keep the summary under {max_tokens} tokens.

Query: {query}

Document: {document}

Relevant Summary:"""
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0.3
    )
    
    return response.choices[0].message.content
```

#### 3. LLMLingua Compression

**Approach:** Use specialized compression models to reduce prompt length while preserving meaning.

```python
from llmlingua import PromptCompressor

compressor = PromptCompressor()

def llmlingua_compress(query, documents, compression_ratio=0.5):
    """Compress context using LLMLingua."""
    # Combine documents
    context = "\n\n".join([doc['text'] for doc in documents])
    
    # Compress
    compressed = compressor.compress_prompt(
        context,
        instruction=query,
        rate=compression_ratio,  # Target 50% compression
        target_token=2000  # Max tokens
    )
    
    return compressed['compressed_prompt']
```

**Benefits:**
- 2-5x compression with minimal information loss
- Preserves key facts and entities
- Reduces LLM costs significantly

**Industrial-Grade Aspect:** For banking, LLMLingua enables including more regulatory context within token limits, improving answer comprehensiveness.

### Combined Reranking + Compression Pipeline

```python
def optimized_rag_pipeline(query, top_k_retrieve=20, top_k_final=5, compress=True):
    """Complete pipeline with reranking and compression."""
    # Step 1: Initial retrieval (cast wide net)
    initial_docs = vector_search(query, top_k=top_k_retrieve)
    
    # Step 2: Rerank to get best candidates
    reranked_docs = rerank_documents(query, initial_docs, top_k=top_k_final)
    
    # Step 3: Compress each document (optional)
    if compress:
        compressed_docs = [
            {
                **doc,
                'text': abstractive_compress(query, doc['text'], max_tokens=200)
            }
            for doc in reranked_docs
        ]
    else:
        compressed_docs = reranked_docs
    
    # Step 4: Build prompt
    context = "\n\n---\n\n".join([
        f"Document {i+1}:\n{doc['text']}"
        for i, doc in enumerate(compressed_docs)
    ])
    
    prompt = f"""Answer the question based on the following context.

Context:
{context}

Question: {query}

Answer:"""
    
    # Step 5: Generate answer
    answer = llm_generate(prompt)
    
    return {
        "answer": answer,
        "sources": compressed_docs,
        "compression_ratio": calculate_compression_ratio(reranked_docs, compressed_docs)
    }
```

### Metadata-Aware Reranking

**Approach:** Incorporate metadata signals into ranking.

```python
def metadata_aware_rerank(query, documents, user_context):
    """Rerank considering metadata and user context."""
    scores = []
    
    for doc in documents:
        # Base semantic score
        base_score = doc['vector_score']
        
        # Metadata boosts
        recency_boost = 1.0
        if 'created_date' in doc['metadata']:
            days_old = (datetime.now() - doc['metadata']['created_date']).days
            recency_boost = 1.0 / (1.0 + days_old / 365)  # Decay over years
        
        authority_boost = 1.0
        if doc['metadata'].get('document_type') == 'regulation':
            authority_boost = 1.5  # Prefer regulations over memos
        
        relevance_boost = 1.0
        if user_context.get('department') == doc['metadata'].get('department'):
            relevance_boost = 1.2  # Prefer same-department docs
        
        # Combined score
        final_score = base_score * recency_boost * authority_boost * relevance_boost
        scores.append((doc, final_score))
    
    # Sort and return
    ranked = sorted(scores, key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked]
```

**Industrial-Grade Aspect:** For banking, metadata-aware reranking ensures:
- Recent regulations override outdated ones
- Official guidance ranks higher than internal memos
- Department-specific policies surface for relevant users

### Performance Optimization

**Batching for Efficiency:**
```python
def batch_rerank(query, documents, batch_size=32):
    """Rerank in batches for efficiency."""
    all_scores = []
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        pairs = [[query, doc['text']] for doc in batch]
        scores = reranker.predict(pairs)
        all_scores.extend(scores)
    
    # Rank all documents
    ranked = sorted(zip(documents, all_scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked]
```

### Evaluation Metrics

**Reranking Quality:**
- **NDCG@K:** Normalized Discounted Cumulative Gain
- **MRR:** Mean Reciprocal Rank
- **Precision@K:** Percentage of relevant docs in top K

**Compression Quality:**
- **Compression Ratio:** Original tokens / Compressed tokens
- **Information Retention:** Percentage of key facts preserved
- **Answer Quality:** Impact on final LLM response quality

### Summary

Contextual compression and reranking optimize RAG retrieval quality. Key takeaways:

- **Reranking improves precision** using cross-encoders or LLMs
- **Compression reduces costs** while preserving relevant information
- **Combined pipelines** (retrieve → rerank → compress) maximize quality
- **Metadata-aware ranking** incorporates domain signals
- **Banking applications** benefit from authority-based ranking and compression of lengthy regulations

Next, we'll explore Self-RAG and Corrective RAG for self-improving systems.
