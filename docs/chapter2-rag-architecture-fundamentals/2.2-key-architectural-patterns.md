---
id: key-architectural-patterns
title: Key Architectural Patterns
---

## 2.2 Key Architectural Patterns

This section will discuss key architectural patterns for RAG, including pros, cons, and best practices for each, with banking context.

### Simple RAG: Direct Retrieval and Generation

The most straightforward RAG pattern involves a direct sequence of steps: retrieve, then generate.

*   **Description:**
    1.  User query is embedded.
    2.  The embedding is used to perform a similarity search in a vector database.
    3.  The top-K most relevant chunks are retrieved.
    4.  These chunks are concatenated with the original query to form an augmented prompt.
    5.  The LLM generates a response based on this augmented prompt.

*   **Pros:**
    *   **Simplicity:** Easy to understand and implement, making it a good starting point for RAG.
    *   **Low Latency:** Fewer sequential steps generally lead to faster response times.
    *   **Reduced Cost:** Less complex processing, potentially lower computational cost.

*   **Cons:**
    *   **Suboptimal Retrieval:** If the initial query is ambiguous or poorly phrased, the retriever might fetch less relevant documents, leading to poorer generation quality.
    *   **Limited Contextual Understanding:** Doesn't account for more complex reasoning or multi-turn conversations.
    *   **Sensitivity to Chunking:** Performance is highly dependent on how well documents are chunked and embedded.

*   **Best Practices for Banking:**
    *   **For well-defined, atomic queries:** Suitable for simple FAQs or direct fact retrieval from a clean, high-quality knowledge base (e.g., "What is the current interest rate for a savings account?").
    *   **High-Quality Embeddings:** Invest in robust embedding models that accurately capture the semantics of banking terminology.
    *   **Clear Chunking Strategy:** Ensure documents are chunked semantically to maximize the chances of retrieving the most relevant information in a single pass.
    *   **Initial Guardrails:** Implement strong LLM guardrails from the outset to prevent generation of financial advice or sensitive data without proper context.

### Multi-stage RAG: Re-ranking, Query Expansion

Multi-stage RAG introduces additional steps in the retrieval process to improve the relevance and quality of the context provided to the LLM.

*   **Description:**
    1.  **Initial Retrieval:** Similar to Simple RAG, an initial set of top-N documents/chunks is retrieved.
    2.  **Re-ranking:** A specialized model (often a cross-encoder or a smaller LLM) re-scores these initial N documents to identify the truly most relevant ones, providing a more precise top-K set.
    3.  **Query Expansion (Optional):** Before initial retrieval, the original user query might be expanded or rephrased using an LLM to generate multiple relevant perspectives, improving recall.
    4.  **Contextual Compression (Optional):** Only the most salient parts of the retrieved documents are passed to the LLM to save tokens and improve focus.

*   **Pros:**
    *   **Improved Relevance:** Re-ranking significantly boosts the precision of retrieved information.
    *   **Enhanced Recall:** Query expansion can help capture a wider range of relevant documents, especially for complex or vague queries.
    *   **Better LLM Performance:** By providing higher-quality, condensed context, the LLM can generate more accurate and focused responses.

*   **Cons:**
    *   **Increased Latency:** Additional processing steps (re-ranking, query expansion) add to the overall response time.
    *   **Higher Computational Cost:** Requires more model inferences.
    *   **Complexity:** More components to manage and optimize.

*   **Best Practices for Banking:**
    *   **For complex queries or diverse knowledge bases:** Highly recommended for banking applications dealing with nuanced financial regulations, customer complaints, or detailed product comparisons.
    *   **Domain-Specific Re-rankers:** Consider fine-tuning a re-ranker on banking-specific data to further improve its ability to prioritize relevant documents.
    *   **Strategic Query Expansion:** Use query expansion judiciously, especially for customer-facing applications, to avoid generating irrelevant sub-queries. Monitor for "query drift."
    *   **Token Optimization:** Crucial for managing costs and LLM context windows, especially with verbose banking documents.

**Code Example: Conceptual Re-ranking with Cohere Rerank (using a mock)**

```python
from typing import List, Dict

# Assume initial retrieval has happened and we have these chunks
initial_retrieved_chunks = [
    {"id": "doc_A", "text": "Details on MyBank's fixed-rate mortgage product, current APR 6.25% (as of Dec 5, 2025)."},
    {"id": "doc_B", "text": "Information about opening a student checking account with a minimum deposit of $25."},
    {"id": "doc_C", "text": "A general article on mortgage market trends in Q4 2025, not specific to MyBank."},
    {"id": "doc_D", "text": "How to apply for a personal loan at MyBank, detailing required documents and credit score implications."},
    {"id": "doc_E", "text": "MyBank's policy on international wire transfers and associated fees."},
]

user_query = "What is MyBank's 30-year fixed mortgage interest rate?"

# Simulate a re-ranking function (in reality, this would be an API call to a re-ranker like Cohere)
def mock_rerank_chunks(query: str, chunks: List[Dict], top_n: int = 2) -> List[Dict]:
    """
    Mock re-ranking based on simple keyword matching for demonstration.
    A real re-ranker uses semantic understanding.
    """
    scored_chunks = []
    for chunk in chunks:
        # Simple scoring: count query keywords in text
        score = sum(1 for word in query.lower().split() if word in chunk["text"].lower())
        if "mybank" in chunk["text"].lower() and "mortgage" in chunk["text"].lower():
            score += 5 # Boost for explicit banking domain keywords
        scored_chunks.append({"chunk": chunk, "score": score})

    # Sort by score in descending order
    scored_chunks.sort(key=lambda x: x["score"], reverse=True)

    # Return only the chunks, not the scores, for simplicity
    return [item["chunk"] for item in scored_chunks[:top_n]]

# --- Example Usage ---
print(f"User Query: '{user_query}'")
print("\n--- Initial Retrieved Chunks ---")
for chunk in initial_retrieved_chunks:
    print(f"- {chunk['text']}")

re_ranked_chunks = mock_rerank_chunks(user_query, initial_retrieved_chunks)

print("\n--- Re-ranked Chunks (Top 2) ---")
for chunk in re_ranked_chunks:
    print(f"- {chunk['text']}")
```

**Expected Output:**

```
User Query: 'What is MyBank's 30-year fixed mortgage interest rate?'

--- Initial Retrieved Chunks ---
- Details on MyBank's fixed-rate mortgage product, current APR 6.25% (as of Dec 5, 2025).
- Information about opening a student checking account with a minimum deposit of $25.
- A general article on mortgage market trends in Q4 2025, not specific to MyBank.
- How to apply for a personal loan at MyBank, detailing required documents and credit score implications.
- MyBank's policy on international wire transfers and associated fees.

--- Re-ranked Chunks (Top 2) ---
- Details on MyBank's fixed-rate mortgage product, current APR 6.25% (as of Dec 5, 2025).
- How to apply for a personal loan at MyBank, detailing required documents and credit score implications.
```

*(Note: A real re-ranker would use a more sophisticated model to understand the semantic similarity, leading to more accurate re-ranking than this keyword-based mock example.)*

### RAG with Query Rewriting and Decomposition

### RAG with Query Rewriting and Decomposition

This pattern focuses on transforming the user's initial query to optimize retrieval, especially for complex or multi-faceted questions.

*   **Description:**
    1.  **Query Rewriting:** An LLM is used to rephrase the original user query into a more retriever-friendly format (e.g., making it more concise, adding keywords, removing conversational filler).
    2.  **Query Decomposition:** For complex questions (e.g., "What are the eligibility criteria for a MyBank credit card, and how do I apply online?"), an LLM breaks it down into multiple simpler sub-queries. Each sub-query is then used for retrieval, and the results are combined.
    3.  **Recursive Retrieval:** A more advanced form where the LLM might retrieve information, analyze it, identify missing pieces, and then generate *new* queries to retrieve more context until it has sufficient information.

*   **Pros:**
    *   **Handles Complex Queries:** Significantly improves retrieval for multi-part or ambiguous questions.
    *   **Enhanced Relevance:** Rewritten queries can better match documents in the vector store.
    *   **Supports Multi-hop Reasoning:** Decomposition enables the system to answer questions that require synthesizing information from multiple distinct sources.

*   **Cons:**
    *   **Higher Latency:** Involves multiple LLM calls for query transformation and potentially multiple retrieval steps.
    *   **Increased Cost:** More LLM inferences.
    *   **Error Propagation:** Errors in query rewriting or decomposition can lead to poor retrieval and generation downstream.
    *   **Complexity:** Requires careful orchestration and prompt engineering for the query transformation LLM.

*   **Best Practices for Banking:**
    *   **For analytical or investigative tasks:** Useful for financial analysts asking complex questions that require data from multiple reports or for compliance teams investigating multi-faceted regulations.
    *   **Rigorous Prompt Engineering:** The LLM performing rewriting/decomposition needs precise instructions. Test prompts extensively.
    *   **Fallback Mechanisms:** If query rewriting fails or produces a nonsensical query, have a fallback to simple RAG or prompt the user for clarification.
    *   **Monitor Query Transformations:** Log the original query, the rewritten/decomposed queries, and the retrieved results to understand and debug the system's behavior.

**Code Example: Conceptual Query Decomposition**

```python
import os
# from openai import OpenAI # Assuming OpenAI is configured
# from dotenv import load_dotenv

# load_dotenv()
# client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

def decompose_query(complex_query: str) -> list[str]:
    """
    Uses an LLM to decompose a complex query into simpler sub-queries.
    This is a conceptual example; a real implementation would use a robust prompt.
    """
    # Simplified mock for demonstration. In reality, a prompt to an LLM:
    # prompt = f"Decompose the following complex query into simpler, independent questions:\n'{complex_query}'\n\nOutput each sub-query on a new line."
    # response = client.chat.completions.create(model="gpt-4", messages=[{"role": "user", "content": prompt}])
    # sub_queries = response.choices[0].message.content.strip().split('\\n')

    if "eligibility criteria for a MyBank credit card, and how do I apply online" in complex_query.lower():
        return [
            "What are the eligibility criteria for a MyBank credit card?",
            "How do I apply for a MyBank credit card online?"
        ]
    elif "loan options and current interest rates" in complex_query.lower():
        return [
            "What are MyBank's personal loan options?",
            "What are the current interest rates for MyBank's personal loans?"
        ]
    else:
        return [complex_query] # Return original if no decomposition rule matches

# --- Example Usage ---
user_query_1 = "What are the eligibility criteria for a MyBank credit card, and how do I apply online?"
sub_queries_1 = decompose_query(user_query_1)
print(f"Original Query 1: '{user_query_1}'")
print(f"Decomposed Sub-Queries 1: {sub_queries_1}")

user_query_2 = "Tell me about MyBank's loan options and current interest rates."
sub_queries_2 = decompose_query(user_query_2)
print(f"\nOriginal Query 2: '{user_query_2}'")
print(f"Decomposed Sub-Queries 2: {sub_queries_2}")
```

**Expected Output:**

```
Original Query 1: 'What are the eligibility criteria for a MyBank credit card, and how do I apply online?'
Decomposed Sub-Queries 1: ['What are the eligibility criteria for a MyBank credit card?', 'How do I apply for a MyBank credit card online?']

Original Query 2: 'Tell me about MyBank's loan options and current interest rates.'
Decomposed Sub-Queries 2: ["What are MyBank's personal loan options?", "What are the current interest rates for MyBank's personal loans?"]
```

### Agentic RAG Frameworks (Introduction)

An LLM is no longer just a generator but an intelligent "agent" that can reason, plan, and use tools—including the retriever—to achieve a goal.

*   **Description:** An LLM acts as an orchestrator, determining when to retrieve information, what to search for, how to process retrieved results, and even when to use other tools (e.g., calculators, APIs). This involves a "chain of thought" or "ReAct" (Reasoning and Acting) pattern.
    1.  Agent receives user query.
    2.  Agent *reasons* about the query and *plans* a sequence of actions.
    3.  Agent *acts* by using tools (e.g., calls the retriever with a specific search query).
    4.  Agent observes the tool's output.
    5.  Agent *reasons* again based on the observation, potentially performing more actions or generating a final response.

*   **Pros:**
    *   **Sophisticated Reasoning:** Can handle highly complex, multi-step queries that require dynamic tool use and decision-making.
    *   **Flexibility:** Adapts its strategy based on the query and retrieved information.
    *   **Reduced Development Overhead (with frameworks):** Frameworks like LangChain and LlamaIndex provide abstractions to build such agents.

*   **Cons:**
    *   **Higher Latency:** Multiple turns of reasoning and tool use can significantly increase response time.
    *   **Non-Determinism:** The agent's behavior can be less predictable, making debugging and quality assurance challenging.
    *   **Increased Cost:** Multiple LLM calls per interaction.
    *   **Complexity:** Requires careful design of tools, prompts, and agent logic.

*   **Best Practices for Banking:**
    *   **For advanced, interactive tasks:** Suitable for financial planning assistants, advanced investigative tools for fraud, or complex policy synthesis requiring dynamic information gathering.
    *   **Well-Defined Tools:** Each tool (including the retriever) should have a clear purpose, input schema, and output format.
    *   **Strong Safety Mechanisms:** Implement robust guardrails at every step of the agent's reasoning and action, especially when interacting with sensitive data or making critical decisions.
    *   **Detailed Logging:** Comprehensive logging of the agent's thought process, tool calls, and observations is critical for auditing and debugging.
    *   **Human Oversight:** For critical banking tasks, human-in-the-loop mechanisms are essential for agent validation and intervention.

**Conceptual Comparison Table (Industrial Perspective):**

| Feature/Pattern         | Simple RAG                          | Multi-stage RAG (Re-ranking/Expansion)       | RAG with Query Rewriting/Decomposition      | Agentic RAG Frameworks                      |
| :---------------------- | :---------------------------------- | :------------------------------------------- | :------------------------------------------ | :------------------------------------------ |
| **Complexity**          | Low                                 | Medium                                       | Medium to High                              | High                                        |
| **Latency**             | Low                                 | Medium                                       | Medium to High                              | High (multiple turns)                       |
| **Cost**                | Low                                 | Medium                                       | Medium to High                              | High                                        |
| **Relevance**           | Good (if query/chunks are clear)    | Very Good (precision boosted)                | Excellent (for complex queries)             | Excellent (dynamic, targeted retrieval)     |
| **Recall**              | Good                                | Improved (with query expansion)              | Excellent (multi-hop)                       | Excellent (adaptive, tool-using)            |
| **Best for Banking**    | Simple FAQs, direct fact lookup     | Nuanced policy retrieval, customer complaints | Complex financial analysis, regulatory interpretation | Financial planning, advanced investigations |
| **Explainability**      | Direct source mapping               | Improved source mapping (post-reranking)     | Traceable to sub-queries and sources        | Traceable to tool calls and reasoning steps |
| **Development Effort**  | Low                                 | Medium                                       | Medium                                      | High                                        |
| **Maintainability**     | Good                                | Medium                                       | Medium                                      | Challenging (non-deterministic)             |

Choosing the right architectural pattern depends on a thorough understanding of your application's requirements, available resources, and tolerance for complexity. For banking, a progressive approach—starting simple and adding complexity as needed—is often a wise strategy, always prioritizing security and accuracy.
