---
id: rag-query-workflow
title: The RAG Query Workflow
---

## 2.4 The RAG Query Workflow

This section will illustrate the step-by-step process from a user query to a response, using a flow chart.

### Step-by-Step Process from User Query to Response

The RAG query workflow can be visualized as a series of interconnected stages, each contributing to the overall outcome.

1.  **User Query Initiation:**
    *   **Description:** The process begins when a user submits a query through a front-end application (e.g., a chatbot, web interface, API call).
    *   **Industrial-Grade Aspect:** User queries can be diverse in language, complexity, and intent. The system must gracefully handle natural language variations, typos, and incomplete information.

2.  **Query Pre-processing (Optional but Recommended):**
    *   **Description:** Before retrieval, the raw user query might undergo transformations to improve retrieval effectiveness. This can include:
        *   **Normalization:** Lowercasing, removing punctuation, correcting common spelling errors.
        *   **Query Expansion:** Using an LLM to generate synonymous queries or expand abbreviations (e.g., "AML" -> "Anti-Money Laundering").
        *   **Query Rewriting/Decomposition:** For complex queries, an LLM might rewrite it into a more concise form or decompose it into multiple simpler sub-queries (as discussed in Chapter 2.2).
    *   **Industrial-Grade Aspect:** This step helps overcome semantic mismatches between user queries and indexed documents. For banking, it ensures that domain-specific jargon or acronyms are correctly interpreted.

3.  **Query Embedding:**
    *   **Description:** The (pre-processed) user query is converted into a high-dimensional vector embedding using an embedding model. This vector captures the semantic meaning of the query.
    *   **Industrial-Grade Aspect:** The choice of embedding model is critical. It should be robust, perform well on your domain-specific language (e.g., financial terminology), and generate embeddings quickly for low latency. Consistency with the embedding model used during data ingestion is paramount.

4.  **Information Retrieval (Search):**
    *   **Description:** The query embedding is used to perform a similarity search against the vector store (knowledge base). The retriever identifies and fetches a set of `N` potentially relevant document chunks.
    *   **Industrial-Grade Aspect:** This step needs to be extremely fast and scalable. It often involves Approximate Nearest Neighbor (ANN) search algorithms. For banking, integrating metadata filtering (e.g., by document type, date, access permissions) at this stage is a best practice to narrow down relevant results and enforce security. Hybrid search (vector + keyword) is often employed to maximize recall.

5.  **Contextual Re-ranking (Optional but Recommended):**
    *   **Description:** The initial `N` retrieved chunks may contain some irrelevant or less salient information. A re-ranker model (often a smaller, specialized cross-encoder LLM) evaluates the relevance of each retrieved chunk *in the context of the original query* and re-scores them, yielding a more precise top `K` (where `K < N`) set of chunks.
    *   **Industrial-Grade Aspect:** This step significantly boosts the precision of the context, improving the quality of the LLM's final response and reducing noise. It's crucial for complex banking documents where subtle differences in meaning can be critical.

6.  **Augmented Prompt Construction:**
    *   **Description:** The refined set of `K` relevant chunks is combined with the original user query (and any system instructions) to form a single, comprehensive "augmented prompt."
    *   **Industrial-Grade Aspect:** This prompt must be carefully engineered. Best practices include:
        *   **Clear Delimiters:** Use specific tokens (e.g., `<context>`, `</context>`) to clearly separate the retrieved information from the user's question, guiding the LLM.
        *   **System Instructions:** Provide explicit instructions to the LLM on how to use the context (e.g., "Answer only from the provided context," "If the answer is not in the context, state that you don't know," "Maintain a professional banking tone").
        *   **Token Management:** Ensure the combined prompt (query + context) fits within the LLM's context window. Implement truncation strategies if necessary.

7.  **LLM Generation:**
    *   **Description:** The augmented prompt is sent to the Large Language Model (generator). The LLM processes this input and generates a natural language response.
    *   **Industrial-Grade Aspect:** The chosen LLM must be powerful enough for the task, adhere to safety and bias guidelines, and provide responses within acceptable latency. For banking, guardrails are essential to prevent generation of inappropriate content or financial advice.

8.  **Output Post-processing (Optional):**
    *   **Description:** The LLM's raw output might be further processed before being presented to the user. This can include:
        *   **Parsing:** Extracting structured data from the free-form text.
        *   **Formatting:** Applying specific formatting rules (e.g., bullet points, bolding).
        *   **Safety Checks:** Running additional checks for harmful content or compliance.
        *   **Citation Generation:** Automatically linking parts of the response back to the source documents for transparency.
    *   **Industrial-Grade Aspect:** Ensures consistency in output format, enhances user experience, and provides an additional layer of safety and compliance. Citation generation is a key best practice for auditability in banking.

9.  **Response Delivery:**
    *   **Description:** The final, processed response is delivered to the user via the user interface or API.
    *   **Industrial-Grade Aspect:** Fast and reliable delivery. User feedback mechanisms (e.g., thumbs up/down) are important for continuous improvement and quality monitoring.

**Conceptual RAG Query Workflow Diagram (Enhanced):**

```mermaid
graph TD
    A[User Query] --> B{Query Pre-processing <br> (Normalization, Expansion, Rewriting)};
    B --> C(Query Embedding);
    C --> D{Information Retrieval <br> (Vector Search + Keyword Search + Metadata Filters)};
    D -- Top N Chunks --> E{Contextual Re-ranking <br> (Optional)};
    E -- Top K Chunks --> F{Augmented Prompt Construction <br> (Query + Context + System Instructions)};
    F --> G(LLM Generation);
    G --> H{Output Post-processing <br> (Parsing, Formatting, Safety, Citations)};
    H --> I[Generated Response & Citations];
    I --> J[User Application/API];
```

This detailed workflow provides a roadmap for building sophisticated RAG systems, allowing for granular control and optimization at each stage to meet stringent industrial demands, especially in highly regulated sectors like banking.
