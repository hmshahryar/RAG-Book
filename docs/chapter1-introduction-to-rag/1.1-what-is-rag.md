---
id: what-is-rag
title: What is RAG?
---

## 1.1 What is RAG?

This section will define RAG and its core concepts, with user-friendly explanations.

### Core Concept

Imagine an LLM as a brilliant student who has read countless books (its training data). While incredibly knowledgeable, this student might struggle with:
1.  **New Information:** Events or facts that occurred *after* their last "reading" session.
2.  **Specific Domain Expertise:** Niche details that weren't extensively covered in their general curriculum.
3.  **Fact-Checking:** Verifying claims with specific, verifiable sources.

RAG addresses these challenges by acting like a research assistant for the LLM. When a question is posed, the RAG system first *retrieves* highly relevant documents or data snippets from a curated, up-to-date knowledge base. These retrieved snippets are then provided to the LLM alongside the original query, allowing the LLM to *generate* an answer that is grounded in real-time or proprietary information.

**Key Idea:** The LLM doesn't "know" the external data; it simply "reads" it as part of its prompt and integrates it into its response. This process significantly reduces the likelihood of hallucinations and enhances the trustworthiness of the generated output.

### Distinction from Traditional LLM Usage

Traditionally, interacting with an LLM often involves simply sending a prompt and receiving a response based on its internal knowledge.

**Traditional LLM Interaction:**

```
User Query -> LLM -> Generated Response
```

In this model, if the information needed is not in the LLM's training data, or if it's outdated, the LLM might:
*   "Hallucinate" a plausible but incorrect answer.
*   State that it doesn't know (less common with highly capable models).
*   Provide a generic answer that lacks specificity.

**RAG-Enhanced LLM Interaction:**

```mermaid
graph TD
    A[User Query] --> B(Retriever);
    B --> C{Knowledge Base <br> (Documents, Vector Store)};
    C --> B;
    B --> D[Retrieved Context];
    D --> E(Augmented Prompt);
    A --> E;
    E --> F[LLM (Generator)];
    F --> G[Generated Response];
```

In the RAG model:
1.  The **User Query** is received.
2.  A **Retriever** component searches a **Knowledge Base** (e.g., a vector database containing embeddings of your company's documents) to find the most relevant pieces of information.
3.  The **Retrieved Context** is then combined with the original **User Query** to form an **Augmented Prompt**.
4.  This richer **Augmented Prompt** is sent to the **LLM (Generator)**.
5.  The LLM generates a **Generated Response** that leverages both its intrinsic language generation capabilities and the provided factual context.

This distinction is crucial for building reliable, domain-specific AI applications, especially in fields like banking where accuracy and adherence to specific, verifiable information are paramount.
