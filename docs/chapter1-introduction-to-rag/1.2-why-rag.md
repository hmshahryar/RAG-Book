---
id: why-rag
title: The "Why" of RAG
---

## 1.2 The "Why" of RAG: Overcoming LLM Limitations

This section will discuss the limitations of traditional LLMs and how RAG addresses them.

### Hallucinations and Factual Inaccuracies

One of the most significant challenges with standalone LLMs is their propensity to "hallucinate"â€”generating plausible-sounding but factually incorrect or nonsensical information. This happens because LLMs are trained to predict the next word based on patterns in their vast training data, not to verify facts against real-world knowledge.

**Industrial-Grade Implication (Banking):** In banking, hallucinations are unacceptable. Imagine an LLM providing incorrect interest rates, misinterpreting a loan agreement, or giving false advice on regulatory compliance. Such errors can lead to severe financial losses, legal repercussions, and a complete erosion of customer trust. RAG mitigates this by grounding responses in verified, external data.

### Lack of Up-to-Date Information and Domain-Specific Knowledge Gaps

LLMs are typically trained on massive datasets collected up to a specific cutoff date. This means they inherently lack knowledge of recent events, new policies, or evolving market conditions. Furthermore, while they have general knowledge, they often lack deep, specific expertise required for highly specialized domains like banking, healthcare, or law.

**Industrial-Grade Implication (Banking):**
*   **Outdated Information:** A bank's policies, product offerings, and regulatory landscape change constantly. An LLM trained two years ago cannot accurately answer questions about today's mortgage rates or the latest anti-money laundering (AML) regulations.
*   **Domain Specificity:** Financial products, legal clauses, and internal operational procedures are highly nuanced. A general-purpose LLM will struggle to provide accurate advice on complex financial instruments or interpret specific clauses in a banking contract without access to the actual documents.

RAG bridges this gap by continuously updating its knowledge base with the latest internal documents, news feeds, and regulatory updates, ensuring the LLM always has access to current and relevant information.

### Transparency and Explainability

Traditional LLMs operate as "black boxes." When they generate an answer, it's often difficult to trace *why* they produced that particular output or *which specific piece of training data* influenced their response. This lack of transparency is a major hurdle for trustworthiness and auditability.

**Industrial-Grade Implication (Banking):** Transparency and explainability are not just desirable but often **mandatory** in banking. Regulators, auditors, and customers demand to know the basis of decisions or information provided. For instance:
*   **Loan Approvals:** If an AI assists in a loan decision, the bank must be able to explain *why* certain information was considered and from which documents it originated.
*   **Compliance Queries:** Answers regarding compliance must be traceable to specific regulatory texts or internal policies.
*   **Customer Support:** Customers need to trust that the information provided is backed by official sources.

RAG inherently improves explainability by allowing developers to show the exact source documents or snippets that were retrieved and used to form the LLM's answer. This "citations" capability is a critical best practice for industrial RAG systems.

**Code Example: Simulating a Hallucination vs. RAG-grounded Response (Conceptual)**

Let's consider a Python example (using `openai` for simplicity) to illustrate the difference.

```python
import os
# from openai import OpenAI # Assume setup with API key

# --- Scenario 1: Standalone LLM (potential for hallucination) ---
print("--- Standalone LLM Response ---")
try:
    # client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    # response = client.chat.completions.create(
    #     model="gpt-4",
    #     messages=[
    #         {"role": "user", "content": "What is the current interest rate for a 30-year fixed mortgage at MyBank, as of today, 2025-12-06?"}
    #     ]
    # )
    # print(response.choices[0].message.content)
    print("LLM might guess or state it doesn't have real-time data. E.g., 'As of my last update, a typical rate might be around 4.5%, but please check MyBank's official website.' This is a potential hallucination or generic advice.")
except Exception as e:
    print(f"Error calling LLM (simulated): {e}")

print("\n" + "="*50 + "\n")

# --- Scenario 2: RAG-Enhanced LLM (grounded in retrieved context) ---
print("--- RAG-Enhanced LLM Response ---")

# In a real RAG system, this would involve a retriever query
retrieved_context = """
As of December 5, 2025, MyBank's current interest rate for a 30-year fixed mortgage is 6.25% APR. This rate is subject to credit approval and may change.
For more details, refer to the 'Residential Loan Products' document, page 15.
"""

# augmented_prompt = f"Given the following information: '{retrieved_context}'\n\nWhat is the current interest rate for a 30-year fixed mortgage at MyBank, as of today, 2025-12-06?"

# try:
#     # response = client.chat.completions.create(
#     #     model="gpt-4",
#     #     messages=[
#     #         {"role": "user", "content": augmented_prompt}
#     #     ]
#     # )
#     # print(response.choices[0].message.content)
print(f"LLM, guided by RAG, provides a specific, verifiable answer:\n'As of December 5, 2025, MyBank\'s 30-year fixed mortgage interest rate is 6.25% APR, as detailed in the 'Residential Loan Products' document. This rate is subject to credit approval and may change.'")
# except Exception as e:
#     print(f"Error calling LLM (simulated): {e}")

```

**Expected Output (Conceptual Simulation):**

```
--- Standalone LLM Response ---
LLM might guess or state it doesn't have real-time data. E.g., 'As of my last update, a typical rate might be around 4.5%, but please check MyBank's official website.' This is a potential hallucination or generic advice.

==================================================

--- RAG-Enhanced LLM Response ---
LLM, guided by RAG, provides a specific, verifiable answer:
'As of December 5, 2025, MyBank\'s 30-year fixed mortgage interest rate is 6.25% APR, as detailed in the 'Residential Loan Products' document. This rate is subject to credit approval and may change.'
```

This conceptual example highlights how RAG transitions LLMs from being potentially unreliable "generalists" to trustworthy, data-grounded "specialists," a critical transformation for enterprise applications.
