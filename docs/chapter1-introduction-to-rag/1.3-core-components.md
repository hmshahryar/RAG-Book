---
id: core-components
title: Core Components of a RAG System
---

## 1.3 Core Components of a RAG System

This section will introduce the main components of a RAG system.

### Retriever: Role and Function

The **Retriever** is the "search engine" of the RAG system. Its primary function is to efficiently locate and extract the most relevant pieces of information from a vast knowledge base that can help answer a user's query.

**Key Responsibilities:**
*   **Query Understanding:** Interprets the user's input to understand its semantic meaning.
*   **Knowledge Base Interaction:** Queries a structured knowledge base (often a vector database) to find relevant documents or chunks.
*   **Relevance Scoring:** Ranks the retrieved documents based on their perceived relevance to the query.

**Industrial-Grade Considerations & Best Practices:**
*   **Efficiency at Scale:** For banking, knowledge bases can contain millions of documents. The retriever must be highly efficient, employing advanced indexing (e.g., Approximate Nearest Neighbor algorithms like HNSW) to return results quickly.
*   **Semantic vs. Keyword Search:** Modern retrievers leverage semantic search (using vector embeddings) to understand the *meaning* of the query, not just exact keyword matches. Often, a hybrid approach combining both is a best practice for robustness.
*   **Contextual Granularity:** The retriever should ideally return small, relevant "chunks" of information rather than entire lengthy documents to avoid overwhelming the LLM's context window.

**Code Example: Basic Retriever Simulation (Conceptual with Faiss/ChromaDB)**

This conceptual example demonstrates how a retriever might work. In a real system, `knowledge_base` would be a vector database.

```python
from typing import List, Dict

# Conceptual representation of our knowledge base (in a real system, this is a vector DB)
# Each 'doc' would have an embedding generated by an embedding model
knowledge_base_chunks = [
    {"id": "doc1", "text": "MyBank offers a 30-year fixed mortgage at 6.25% APR as of Dec 5, 2025. This is for residential properties."},
    {"id": "doc2", "text": "To open a new checking account, you need a valid ID and proof of address. Minimum deposit is $50."},
    {"id": "doc3", "text": "MyBank's credit card interest rates vary from 15% to 22% APR based on credit score."},
    {"id": "doc4", "text": "New AML (Anti-Money Laundering) regulations require enhanced due diligence for transactions over $10,000."},
    {"id": "doc5", "text": "Our customer service is available 24/7 via phone at 1-800-MYBANK or chat on our website."},
]

# Simulate an embedding function (in reality, this would be an actual embedding model)
def get_embedding(text: str) -> List[float]:
    """Generates a dummy embedding for demonstration purposes."""
    # A real embedding would be a dense vector, e.g., from OpenAI, Cohere, Sentence-BERT
    return [len(text) / 100.0, sum(ord(c) for c in text) / 10000.0] # Dummy example

# Simulate a simple vector similarity search (conceptual)
def retrieve_relevant_chunks(query: str, top_k: int = 2) -> List[Dict]:
    query_embedding = get_embedding(query)

    # In a real system, this would be a vector database query
    # For demonstration, we'll just "simulate" relevance
    if "mortgage" in query.lower() or "loan" in query.lower():
        return [knowledge_base_chunks[0]] # Return mortgage info
    elif "account" in query.lower() or "deposit" in query.lower():
        return [knowledge_base_chunks[1]] # Return checking account info
    elif "customer service" in query.lower() or "contact" in query.lower():
        return [knowledge_base_chunks[4]] # Return customer service info
    else:
        # Fallback for other queries, might return less relevant
        return knowledge_base_chunks[:top_k] # Simple top-k for demo

# --- Example Usage ---
user_query_1 = "What are the current mortgage rates?"
retrieved_1 = retrieve_relevant_chunks(user_query_1)
print(f"Query: '{user_query_1}'")
print(f"Retrieved: {[chunk['text'] for chunk in retrieved_1]}")

user_query_2 = "How do I open a checking account?"
retrieved_2 = retrieve_relevant_chunks(user_query_2)
print(f"\nQuery: '{user_query_2}'")
print(f"Retrieved: {[chunk['text'] for chunk in retrieved_2]}")
```

**Expected Output:**

```
Query: 'What are the current mortgage rates?'
Retrieved: ["MyBank offers a 30-year fixed mortgage at 6.25% APR as of Dec 5, 2025. This is for residential properties."]

Query: 'How do I open a checking account?'
Retrieved: ["To open a new checking account, you need a valid ID and proof of address. Minimum deposit is $50."]
```

### Generator: Role and Function

The **Generator** is typically a Large Language Model (LLM) itself. Its role is to synthesize the user's query and the context provided by the retriever into a coherent, accurate, and natural-sounding response.

**Key Responsibilities:**
*   **Contextual Synthesis:** Combines the retrieved information with its own general knowledge to formulate an answer.
*   **Natural Language Generation:** Produces human-readable text that directly addresses the user's query.
*   **Adherence to Context:** Crucially, it should prioritize and faithfully reflect the information present in the retrieved context, minimizing hallucination.

**Industrial-Grade Considerations & Best Practices:**
*   **Prompt Engineering:** The way the retrieved context is presented to the LLM (the "augmented prompt") is critical. Best practices involve clear instructions, specifying desired output format, and using delimiters to separate context from the main query.
*   **Model Selection:** Choosing the right LLM (e.g., GPT-4, Claude 3, Llama 3, Gemini) depends on factors like performance, cost, security requirements, and the complexity of the task.
*   **Safety and Guardrails:** For banking, ensuring the generated response is safe, avoids harmful content, and does not provide financial advice outside its scope is paramount.
*   **Conciseness and Accuracy:** The generator should aim for clear, concise answers, avoiding verbosity while ensuring all relevant facts from the retrieved context are included.

### Orchestrator/Controller

The **Orchestrator**, sometimes referred to as the Controller or Agent, manages the overall flow of the RAG system. It coordinates the interactions between the user, the retriever, and the generator.

**Key Responsibilities:**
*   **Workflow Management:** Directs the user's query to the retriever, passes the retrieved context to the generator, and delivers the final response back to the user.
*   **Error Handling:** Manages failures in retrieval or generation steps.
*   **Post-processing (Optional):** May perform additional steps like re-ranking retrieved documents, filtering LLM output, or formatting the final response.

**Industrial-Grade Considerations & Best Practices:**
*   **Robustness:** The orchestrator must be resilient to failures in individual components, providing graceful degradation or informative error messages.
*   **Flexibility:** Should allow for easy modification of the RAG pipeline, such as adding re-ranking steps, query transformations, or different LLM calls.
*   **Observability:** Should log key events and metrics (e.g., retrieval latency, LLM token usage, quality scores) to enable monitoring and debugging.
*   **Framework Utilization:** Frameworks like LangChain or LlamaIndex are common best practices for building robust and flexible RAG orchestrators, abstracting much of the complexity.

**Conceptual Diagram of Core Components:**

```mermaid
graph TD
    A[User Application/API] --> B(Orchestrator/Controller);
    B -- Query --> C(Retriever);
    C -- Search --> D{Knowledge Base <br> (Vector Store, Documents)};
    D -- Retrieved Chunks --> C;
    C -- Relevant Context --> B;
    B -- Augmented Prompt --> E(Generator <br> (LLM));
    E -- Generated Response --> B;
    B -- Final Response --> A;
```

Understanding how these components interact is key to building an effective RAG system that delivers accurate and reliable information, particularly in demanding environments like the financial sector.
