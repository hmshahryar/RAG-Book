---
id: rag-evaluation-frameworks
title: RAG Evaluation Frameworks (RAGAS, TruLens)
---

## 6.1 RAG Evaluation Frameworks (RAGAS, TruLens)

Evaluating RAG systems requires specialized frameworks that assess both retrieval quality and generation accuracy. This section covers RAGAS and TruLens, the leading evaluation frameworks.

### RAGAS (RAG Assessment)

**Key Metrics:**
1. **Faithfulness**: Answer supported by context
2. **Answer Relevance**: Answer addresses the query
3. **Context Precision**: Relevant chunks ranked highly
4. **Context Recall**: All relevant info retrieved

#### Installation and Setup

```python
pip install ragas

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)

# Prepare evaluation dataset
eval_data = {
    "question": ["What is the Basel III capital requirement?"],
    "answer": ["The minimum CET1 ratio is 4.5%"],
    "contexts": [["Basel III requires 4.5% CET1 ratio..."]],
    "ground_truths": [["4.5% CET1"]]
}

# Evaluate
result = evaluate(
    eval_data,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)

print(result)
# Output: {'faithfulness': 0.95, 'answer_relevancy': 0.92, ...}
```

### TruLens for Observability

```python
from trulens_eval import TruChain, Feedback, Tru

# Initialize
tru = Tru()

# Define feedback functions
f_qa_relevance = Feedback(openai.relevance).on_input_output()
f_qs_relevance = Feedback(openai.qs_relevance).on_input().on(context)

# Wrap RAG chain
tru_recorder = TruChain(
    rag_chain,
    app_id="banking_rag",
    feedbacks=[f_qa_relevance, f_qs_relevance]
)

# Run and track
with tru_recorder as recording:
    response = rag_chain.run(query)

# View dashboard
tru.run_dashboard()
```

### Custom Evaluation Framework

```python
class BankingRAGEvaluator:
    def __init__(self):
        self.metrics = {}
    
    def evaluate_faithfulness(self, answer, context):
        """Check if answer is supported by context."""
        prompt = f"""Is this answer fully supported by the context?
Context: {context}
Answer: {answer}
Response (yes/no):"""
        
        result = llm_generate(prompt)
        return 1.0 if "yes" in result.lower() else 0.0
    
    def evaluate_citation_quality(self, answer):
        """Check if answer includes proper citations."""
        citation_pattern = r'\[Source:.*?\]'
        citations = re.findall(citation_pattern, answer)
        return len(citations) > 0
    
    def evaluate_compliance(self, answer):
        """Check if answer maintains compliance tone."""
        compliance_keywords = ["regulation", "requirement", "must", "shall"]
        score = sum(1 for kw in compliance_keywords if kw in answer.lower())
        return min(score / 2, 1.0)  # Normalize to 0-1
    
    def evaluate(self, query, answer, context, ground_truth):
        """Complete evaluation."""
        return {
            "faithfulness": self.evaluate_faithfulness(answer, context),
            "citation_quality": self.evaluate_citation_quality(answer),
            "compliance_tone": self.evaluate_compliance(answer),
            "bleu_score": self.calculate_bleu(answer, ground_truth)
        }
```

### Summary

RAGAS and TruLens provide comprehensive RAG evaluation. Custom metrics can address domain-specific needs like banking compliance.
