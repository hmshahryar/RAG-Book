---
id: chain-of-thought-reasoning
title: Chain-of-Thought and Reasoning Prompts
---

## 5.4 Chain-of-Thought and Reasoning Prompts

Chain-of-thought (CoT) prompting encourages LLMs to show their reasoning process, improving accuracy for complex queries.

### Basic CoT Pattern

```python
def cot_rag_prompt(query, context):
    """Chain-of-thought RAG prompt."""
    
    prompt = f"""Answer the question using step-by-step reasoning.

Context:
{context}

Question: {query}

Let's think step by step:
1. First, identify relevant information from the context
2. Then, analyze how it relates to the question
3. Finally, formulate a clear answer

Reasoning:"""
    
    return prompt
```

### Banking Compliance CoT

```python
COMPLIANCE_COT_TEMPLATE = """Assess compliance using this framework:

Context: {context}

Question: {query}

Step 1 - Identify Requirements:
[List applicable regulations and requirements]

Step 2 - Analyze Current State:
[Evaluate current situation against requirements]

Step 3 - Determine Compliance Status:
[Compliant / Non-Compliant / Partial with explanation]

Step 4 - Recommendations:
[Actions needed if non-compliant]

Assessment:"""
```

### Self-Consistency

```python
def self_consistency_rag(query, context, n=5):
    """Generate multiple reasoning paths and select most consistent."""
    
    answers = []
    for _ in range(n):
        answer = llm_generate(cot_rag_prompt(query, context), temperature=0.7)
        answers.append(answer)
    
    # Select most common answer
    from collections import Counter
    answer_counts = Counter(answers)
    most_common = answer_counts.most_common(1)[0][0]
    
    return most_common
```

### Summary

Chain-of-thought prompting improves reasoning quality for complex banking queries. Self-consistency further enhances reliability.

Next: Prompt security and injection prevention.
