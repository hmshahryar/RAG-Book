---
id: prompt-design-patterns
title: Effective Prompt Design Patterns
---

## 5.1 Effective Prompt Design Patterns

Prompt engineering is critical for RAG systemsâ€”the quality of your prompts directly impacts answer accuracy, consistency, and user satisfaction. This section covers proven prompt patterns for production RAG systems in banking and enterprise environments.

### Core Prompt Components for RAG

**Essential Elements:**
1. **System Role**: Define the assistant's identity and constraints
2. **Context Injection**: Provide retrieved documents clearly
3. **User Query**: The actual question to answer
4. **Instructions**: How to use context and format output
5. **Output Format**: Structure expectations

### Pattern 1: Basic RAG Prompt Template

```python
def basic_rag_prompt(query, retrieved_docs):
    """Standard RAG prompt pattern."""
    
    context = "\n\n".join([
        f"Document {i+1}:\n{doc['text']}"
        for i, doc in enumerate(retrieved_docs)
    ])
    
    prompt = f"""You are a helpful banking assistant. Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer:"""
    
    return prompt
```

**Industrial-Grade Aspect:** For banking, always specify that answers must be based on provided context to prevent hallucinations about regulations or financial data.

### Pattern 2: Structured Context with Delimiters

```python
def structured_rag_prompt(query, retrieved_docs):
    """Use clear delimiters to separate context from query."""
    
    context_parts = []
    for i, doc in enumerate(retrieved_docs):
        context_parts.append(f"""
<document id="{doc['id']}">
<title>{doc['metadata']['title']}</title>
<source>{doc['metadata']['source']}</source>
<content>
{doc['text']}
</content>
</document>
""")
    
    prompt = f"""You are a banking compliance assistant with expertise in regulatory requirements.

<context>
{' '.join(context_parts)}
</context>

<question>
{query}
</question>

<instructions>
1. Answer based ONLY on the information in the context documents
2. Cite specific documents using their IDs
3. If the answer is not in the context, state "The provided documents do not contain this information"
4. Maintain a professional, precise tone
</instructions>

<answer>
"""
    
    return prompt
```

**Benefits:**
- Clear separation prevents context bleeding
- LLM can easily identify different sections
- Reduces prompt injection risks

### Pattern 3: Few-Shot RAG Prompt

```python
def few_shot_rag_prompt(query, retrieved_docs):
    """Include examples of desired output format."""
    
    examples = """
Example 1:
Question: What is the minimum tier 1 capital ratio under Basel III?
Context: Basel III requires banks to maintain a minimum Common Equity Tier 1 (CET1) ratio of 4.5%...
Answer: According to Basel III regulations, the minimum Common Equity Tier 1 (CET1) capital ratio is 4.5% of risk-weighted assets. [Source: Basel III Framework, Section 2.1]

Example 2:
Question: What are the stress testing requirements for large banks?
Context: The Dodd-Frank Act requires banks with assets over $100 billion to undergo annual stress tests...
Answer: Under the Dodd-Frank Act, banks with total consolidated assets exceeding $100 billion must conduct annual stress tests to assess their capital adequacy under adverse economic scenarios. [Source: Dodd-Frank Act, Title I, Section 165]
"""
    
    context = format_documents(retrieved_docs)
    
    prompt = f"""You are a banking regulatory expert. Answer questions by citing specific regulations.

{examples}

Now answer this question:

Context:
{context}

Question: {query}

Answer:"""
    
    return prompt
```

### Pattern 4: Chain-of-Thought RAG

```python
def cot_rag_prompt(query, retrieved_docs):
    """Encourage step-by-step reasoning."""
    
    context = format_documents(retrieved_docs)
    
    prompt = f"""You are a banking analyst. Answer the question using step-by-step reasoning.

Context:
{context}

Question: {query}

Instructions:
1. First, identify the relevant information from the context
2. Then, reason through the answer step-by-step
3. Finally, provide a clear, concise answer

Reasoning:"""
    
    return prompt
```

**Use Case:** Complex queries requiring multi-step analysis (e.g., calculating compliance ratios, comparing regulations).

### Pattern 5: Constrained Output Format

```python
def structured_output_rag_prompt(query, retrieved_docs):
    """Enforce specific output format."""
    
    context = format_documents(retrieved_docs)
    
    prompt = f"""You are a banking compliance assistant. Provide answers in the specified JSON format.

Context:
{context}

Question: {query}

Output Format (JSON):
{{
  "answer": "Direct answer to the question",
  "confidence": "high|medium|low",
  "sources": ["List of cited document IDs"],
  "relevant_regulations": ["List of specific regulations mentioned"],
  "caveats": ["Any important caveats or limitations"]
}}

Response:"""
    
    return prompt
```

**Industrial-Grade Aspect:** Structured outputs enable downstream processing, validation, and integration with banking systems.

### Pattern 6: Multi-Turn Conversation RAG

```python
def conversational_rag_prompt(query, retrieved_docs, conversation_history):
    """Handle multi-turn conversations with context."""
    
    # Format conversation history
    history = "\n".join([
        f"User: {turn['query']}\nAssistant: {turn['response']}"
        for turn in conversation_history
    ])
    
    context = format_documents(retrieved_docs)
    
    prompt = f"""You are a banking assistant engaged in a conversation. Use the conversation history and new context to answer.

Previous Conversation:
{history}

New Context (for current question):
{context}

Current Question: {query}

Instructions:
- Reference previous conversation when relevant
- Use new context to provide updated or additional information
- Maintain conversation continuity

Answer:"""
    
    return prompt
```

### Pattern 7: Comparative Analysis Prompt

```python
def comparative_rag_prompt(query, docs_set_a, docs_set_b, comparison_aspect):
    """Compare information from two document sets."""
    
    context_a = format_documents(docs_set_a)
    context_b = format_documents(docs_set_b)
    
    prompt = f"""You are a regulatory analyst. Compare the following two sets of documents.

Set A (e.g., Basel II):
{context_a}

Set B (e.g., Basel III):
{context_b}

Question: {query}

Instructions:
1. Identify key differences in {comparison_aspect}
2. Highlight similarities
3. Explain the implications of changes
4. Provide a summary comparison table

Analysis:"""
    
    return prompt
```

### Banking-Specific Prompt Templates

#### Template 1: Regulatory Compliance Query

```python
COMPLIANCE_TEMPLATE = """You are a banking compliance officer assistant. Answer regulatory compliance questions with precision.

Regulatory Context:
{context}

Compliance Question: {query}

Required Output:
1. Direct Answer: [Yes/No/Partial compliance]
2. Relevant Regulations: [List specific sections]
3. Requirements: [What must be done]
4. Deadlines: [If applicable]
5. Penalties for Non-Compliance: [If mentioned]
6. Recommendations: [Best practices]

Response:"""
```

#### Template 2: Risk Assessment Query

```python
RISK_ASSESSMENT_TEMPLATE = """You are a risk management analyst. Assess risks based on the provided information.

Risk Context:
{context}

Risk Question: {query}

Assessment Framework:
1. Risk Identification: [What risks are present]
2. Risk Level: [High/Medium/Low with justification]
3. Mitigation Strategies: [From context]
4. Regulatory Requirements: [Relevant regulations]
5. Monitoring Recommendations: [How to track]

Risk Assessment:"""
```

#### Template 3: Financial Calculation Query

```python
CALCULATION_TEMPLATE = """You are a financial analyst. Perform calculations based on provided data.

Financial Data:
{context}

Calculation Request: {query}

Output Format:
1. Formula Used: [Show the formula]
2. Input Values: [List values from context]
3. Calculation Steps: [Show work]
4. Final Result: [Answer with units]
5. Interpretation: [What this means]
6. Regulatory Threshold: [If applicable]

Calculation:"""
```

### Prompt Optimization Techniques

#### 1. Instruction Clarity

**Bad:**
```
Answer the question using the context.
```

**Good:**
```
Answer the question using ONLY the information provided in the context documents. If the answer is not explicitly stated in the context, respond with "This information is not available in the provided documents." Do not use external knowledge or make assumptions.
```

#### 2. Output Constraints

```python
def add_output_constraints(base_prompt):
    """Add constraints to prevent unwanted behaviors."""
    
    constraints = """
Output Constraints:
- Maximum length: 300 words
- Do not include disclaimers like "I'm an AI" or "I cannot provide financial advice"
- Use bullet points for lists
- Bold key terms
- Include citations in [Source: Document ID] format
"""
    
    return base_prompt + "\n" + constraints
```

#### 3. Tone and Style Specification

```python
BANKING_TONE_SPEC = """
Tone and Style:
- Professional and authoritative
- Precise and unambiguous
- Avoid casual language
- Use banking terminology correctly
- Be concise but comprehensive
- Maintain objectivity
"""
```

### Prompt Testing and Iteration

```python
def test_prompt_variations(query, retrieved_docs, prompt_templates):
    """Test multiple prompt variations."""
    
    results = []
    
    for template_name, template_func in prompt_templates.items():
        prompt = template_func(query, retrieved_docs)
        response = llm_generate(prompt)
        
        # Evaluate response
        score = evaluate_response(query, response, retrieved_docs)
        
        results.append({
            "template": template_name,
            "response": response,
            "score": score
        })
    
    # Return best performing template
    best = max(results, key=lambda x: x['score'])
    return best
```

### Best Practices

1. **Be Explicit**: Clearly state what the LLM should and shouldn't do
2. **Use Delimiters**: Separate context, query, and instructions
3. **Provide Examples**: Few-shot examples improve consistency
4. **Specify Format**: Define expected output structure
5. **Add Constraints**: Prevent unwanted behaviors
6. **Test Variations**: A/B test different prompts
7. **Version Control**: Track prompt changes like code

### Summary

Effective prompt design is crucial for RAG quality. Key takeaways:

- **Structured prompts** with clear delimiters improve reliability
- **Few-shot examples** ensure consistent output format
- **Banking-specific templates** address domain requirements
- **Output constraints** prevent hallucinations and ensure compliance
- **Iterative testing** identifies optimal prompt patterns

Next, we'll explore context window optimization techniques.
