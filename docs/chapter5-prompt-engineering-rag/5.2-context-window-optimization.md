---
id: context-window-optimization
title: Context Window Optimization
---

## 5.2 Context Window Optimization

LLMs have finite context windows (typically 4K-200K tokens). Optimizing how you use this limited space is critical for RAG performance, especially when dealing with lengthy banking documents and regulations.

### Understanding Token Budgets

**Typical Context Windows:**
- GPT-3.5-turbo: 16K tokens (~12K words)
- GPT-4: 8K-128K tokens (6K-96K words)
- Claude 3: 200K tokens (~150K words)
- Gemini 1.5 Pro: 1M tokens (~750K words)

**Token Allocation Strategy:**
```python
def calculate_token_budget(model="gpt-4", safety_margin=0.1):
    """Calculate available tokens for context."""
    
    context_limits = {
        "gpt-3.5-turbo": 16384,
        "gpt-4": 8192,
        "gpt-4-turbo": 128000,
        "claude-3-opus": 200000,
        "gemini-1.5-pro": 1000000
    }
    
    max_tokens = context_limits.get(model, 8192)
    
    # Reserve tokens for different parts
    system_prompt_tokens = 200
    user_query_tokens = 100
    output_tokens = 1000  # Reserve for generation
    safety_buffer = int(max_tokens * safety_margin)
    
    available_for_context = max_tokens - (
        system_prompt_tokens + 
        user_query_tokens + 
        output_tokens + 
        safety_buffer
    )
    
    return {
        "total": max_tokens,
        "available_for_context": available_for_context,
        "reserved_for_output": output_tokens
    }
```

### Dynamic Context Truncation

#### Strategy 1: Top-K Selection

```python
def top_k_truncation(retrieved_docs, query, max_tokens=4000):
    """Select top-K documents that fit within token budget."""
    
    selected_docs = []
    current_tokens = 0
    
    for doc in retrieved_docs:
        doc_tokens = count_tokens(doc['text'])
        
        if current_tokens + doc_tokens <= max_tokens:
            selected_docs.append(doc)
            current_tokens += doc_tokens
        else:
            break  # Stop when budget exceeded
    
    return selected_docs
```

#### Strategy 2: Proportional Allocation

```python
def proportional_truncation(retrieved_docs, max_tokens=4000):
    """Allocate tokens proportionally based on relevance scores."""
    
    total_score = sum(doc['score'] for doc in retrieved_docs)
    
    for doc in retrieved_docs:
        # Allocate tokens based on relevance
        doc_budget = int((doc['score'] / total_score) * max_tokens)
        doc['allocated_tokens'] = doc_budget
        
        # Truncate document to allocated budget
        doc['truncated_text'] = truncate_to_tokens(doc['text'], doc_budget)
    
    return retrieved_docs
```

#### Strategy 3: Sliding Window

```python
def sliding_window_context(long_document, query, window_size=2000, stride=1000):
    """Process long documents using sliding windows."""
    
    doc_tokens = tokenize(long_document)
    windows = []
    
    for i in range(0, len(doc_tokens), stride):
        window = doc_tokens[i:i + window_size]
        window_text = detokenize(window)
        
        # Score window relevance to query
        relevance = calculate_relevance(window_text, query)
        
        windows.append({
            "text": window_text,
            "start_pos": i,
            "relevance": relevance
        })
    
    # Return most relevant windows
    windows.sort(key=lambda x: x['relevance'], reverse=True)
    return windows[:3]  # Top 3 windows
```

### Intelligent Chunk Prioritization

```python
def prioritize_chunks(chunks, query, user_context):
    """Prioritize chunks based on multiple factors."""
    
    for chunk in chunks:
        score = 0
        
        # Factor 1: Semantic relevance (from vector search)
        score += chunk['vector_score'] * 0.4
        
        # Factor 2: Recency
        days_old = (datetime.now() - chunk['metadata']['date']).days
        recency_score = 1.0 / (1.0 + days_old / 365)
        score += recency_score * 0.2
        
        # Factor 3: Authority (document type)
        authority_weights = {
            "regulation": 1.0,
            "policy": 0.8,
            "memo": 0.5,
            "email": 0.3
        }
        authority = authority_weights.get(chunk['metadata']['type'], 0.5)
        score += authority * 0.2
        
        # Factor 4: User department match
        if chunk['metadata']['department'] == user_context.get('department'):
            score += 0.2
        
        chunk['priority_score'] = score
    
    # Sort by priority
    chunks.sort(key=lambda x: x['priority_score'], reverse=True)
    return chunks
```

### Context Compression Techniques

#### 1. Extractive Summarization

```python
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def compress_context(documents, target_tokens=2000):
    """Compress documents to fit token budget."""
    
    compressed_docs = []
    
    for doc in documents:
        doc_tokens = count_tokens(doc['text'])
        
        if doc_tokens > 500:  # Only summarize long docs
            # Calculate compression ratio
            ratio = min(target_tokens / doc_tokens, 0.5)
            max_length = int(doc_tokens * ratio)
            
            summary = summarizer(
                doc['text'],
                max_length=max_length,
                min_length=50,
                do_sample=False
            )[0]['summary_text']
            
            compressed_docs.append({
                **doc,
                'text': summary,
                'compressed': True
            })
        else:
            compressed_docs.append(doc)
    
    return compressed_docs
```

#### 2. LLMLingua for Prompt Compression

```python
from llmlingua import PromptCompressor

compressor = PromptCompressor()

def compress_with_llmlingua(context, query, compression_rate=0.5):
    """Use LLMLingua for intelligent compression."""
    
    compressed = compressor.compress_prompt(
        context,
        instruction=query,
        rate=compression_rate,
        target_token=2000
    )
    
    return compressed['compressed_prompt']
```

### Adaptive Context Selection

```python
def adaptive_context_selection(query, retrieved_docs, model="gpt-4"):
    """Adapt context selection based on query complexity."""
    
    # Classify query complexity
    complexity = classify_query_complexity(query)
    
    budget = calculate_token_budget(model)
    
    if complexity == "simple":
        # Simple query: use fewer, more relevant docs
        selected = top_k_truncation(retrieved_docs[:3], query, budget['available_for_context'])
    
    elif complexity == "moderate":
        # Moderate: balance breadth and depth
        selected = top_k_truncation(retrieved_docs[:5], query, budget['available_for_context'])
    
    else:  # complex
        # Complex: maximize context, use compression
        compressed = compress_context(retrieved_docs[:10], budget['available_for_context'])
        selected = compressed
    
    return selected

def classify_query_complexity(query):
    """Classify query complexity."""
    
    # Simple heuristics
    if len(query.split()) < 10:
        return "simple"
    elif "compare" in query.lower() or "difference" in query.lower():
        return "complex"
    else:
        return "moderate"
```

### Hierarchical Context Organization

```python
def hierarchical_context(query, documents):
    """Organize context hierarchically for better token usage."""
    
    prompt = f"""Question: {query}

Primary Sources (Most Relevant):
"""
    
    # Tier 1: Top 2 most relevant (full text)
    for i, doc in enumerate(documents[:2]):
        prompt += f"\n[Source {i+1}] {doc['metadata']['title']}\n{doc['text']}\n"
    
    # Tier 2: Next 3 (summaries)
    prompt += "\nSupporting Sources (Summaries):\n"
    for i, doc in enumerate(documents[2:5], start=3):
        summary = summarize_document(doc['text'], max_length=200)
        prompt += f"\n[Source {i}] {doc['metadata']['title']}: {summary}\n"
    
    # Tier 3: Additional context (titles only)
    if len(documents) > 5:
        prompt += "\nAdditional References:\n"
        for i, doc in enumerate(documents[5:10], start=6):
            prompt += f"[Source {i}] {doc['metadata']['title']}\n"
    
    prompt += "\nAnswer:"
    
    return prompt
```

### Best Practices for Banking RAG

```python
def banking_context_optimization(query, documents, user_role):
    """Optimize context for banking queries."""
    
    # 1. Prioritize regulatory documents
    docs_sorted = sorted(
        documents,
        key=lambda x: (
            x['metadata'].get('is_regulation', False),
            x['score']
        ),
        reverse=True
    )
    
    # 2. Ensure critical sections are included
    critical_sections = extract_critical_sections(docs_sorted, query)
    
    # 3. Add compliance context
    compliance_note = """
IMPORTANT: This answer must comply with banking regulations. 
If uncertain, err on the side of caution and recommend consulting compliance team.
"""
    
    # 4. Build optimized prompt
    budget = calculate_token_budget()
    selected_docs = top_k_truncation(docs_sorted, query, budget['available_for_context'] - 100)
    
    context = format_documents(selected_docs)
    
    prompt = f"""{compliance_note}

Context:
{context}

Question: {query}

Answer (cite specific regulations):"""
    
    return prompt
```

### Monitoring Token Usage

```python
import tiktoken

def monitor_token_usage(prompt, model="gpt-4"):
    """Monitor and log token usage."""
    
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(prompt)
    
    metrics = {
        "total_tokens": len(tokens),
        "model": model,
        "timestamp": datetime.now().isoformat()
    }
    
    # Log for cost tracking
    log_token_usage(metrics)
    
    # Alert if approaching limit
    model_limit = get_model_limit(model)
    if len(tokens) > model_limit * 0.9:
        alert(f"Token usage at {len(tokens)}/{model_limit} (90%)")
    
    return metrics
```

### Summary

Context window optimization is essential for cost-effective, high-quality RAG. Key takeaways:

- **Token budgeting** prevents exceeding context limits
- **Dynamic truncation** adapts to available space
- **Prioritization** ensures most relevant content is included
- **Compression** techniques maximize information density
- **Hierarchical organization** balances detail and breadth
- **Monitoring** tracks usage and costs

Next, we'll explore few-shot learning techniques for RAG.
